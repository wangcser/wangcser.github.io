<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从 HTTP 到 HTTPS]]></title>
    <url>%2F2019%2F07%2F28%2Fhttps%2F</url>
    <content type="text"><![CDATA[从 HTTP 到 HTTPsHTTP 协议存在的安全问题： 明文传输，存在窃听风险 不验证通信双方的身份，存在伪装风险 无法证明报文的完整性，存在篡改风险 HTTPs 指使用 SSL（Secure Sockets Layer）子层进行的 HTTP 通信，SSL 为 HTTP 提供了加密、认证和放篡改的功能。 对称秘钥加密 Symmetric Key Encryption 特点：加密解密使用同样的秘钥 优点：运算速度快 缺点： 无法借助不安全的信道安全地将秘钥传输给对端 任一秘钥暴露则整个加密系统失效 非对称加密 / 公钥加密 Public Key Encryption 特点： 加密和解密使用不同的秘钥 加密用途：公钥所有人都可获得，公钥持有者可以使用公钥对消息进行加密，只有对应私钥持有者才能对密文进行解密 签名用途：私钥持有者对消息进行签名，公钥持有者可以使用公钥借助签名、原文验证签名的真伪，这里的真伪是指： 认证 篡改 抵赖 优点：可以借助不安全的信道将公钥发送出去 缺点：运算速度慢 HTTPS 的加密方式HTTPS 协议采用混合加密机制： 使用非对称加密传输对称秘钥 —— 保证秘钥分发的安全性 使用对称秘钥通信 —— 保证通信过程的效率 建立连接的通信流程如下： 客户端向服务器请求建立加密链接 服务器将自己的非对称公钥发送给客户端 客户端在本地生成对称秘钥，使用服务器的公钥将对称秘钥加密成密文发送给服务器 服务器使用自己的私钥对密文进行解密，得到客户端的对称秘钥 双方基于对称秘钥进行通信 上面的过程解决的两个问题在于： 使用密文进行通信保证了通信过程无法被窃听 通信过程中不仅要发送可恢复的密文，也要发送消息的摘要，摘要用于保证消息的完整性和不可篡改性 上面的过程中存在的一个漏洞在于： 完全可以有人来假冒服务器，因此需要对步骤 2 中服务器的公钥进行认证 公钥认证HTTPS 通信使用证书来对通信方进行认证，这个过程中需要由可信第三方颁发的证书来对持有的身份进行背书，CA（Certificate Authority）是客户端和服务器都可信赖的第三方机构。 下面是服务器向 CA 申请证书的简单流程： 服务器运营人员向 CA 提出公开秘钥申请，提供相关的物理资料供 CA 审核； CA 判明申请者身份后 对其申请的公钥进行数字签名 并分配这个已签名的公钥 将公钥和签名绑定 将包含公钥的证书交给服务器，服务器之后的通信由发送公钥变为发送证书 消息完整性保护HTTPS 中使用报文摘要技术来保证消息的完整性，结合加密和认证操作后摘要功能变得很安全 HTTPS 的缺点 引入加解密，速度更慢 需要支付证书授权的高额费用]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>HTTPs</tag>
        <tag>对称加密</tag>
        <tag>非对称加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈溢出问题]]></title>
    <url>%2F2019%2F07%2F28%2Foverflow%2F</url>
    <content type="text"><![CDATA[溢出的分类 算术溢出：在单次计算中，计算的结果大于其被分配的存储空间所能存储或表示的能力限制 缓冲区溢出：指向缓冲区写入内容使之溢出的情况 堆栈溢出：指过多次的函数调用使调用栈溢出的情况 在 DSA 领域最容易出现的是算术溢出，下面来详细分析一下。 算术溢出当程序中的数据超过其数据类型的范围时，就会产生溢出，整数类型的溢出称为整数溢出。 原理希望分析溢出的原理，除了从原码-反码-补码的角度，还可以从机器码的角度入手，下面的分析从机器码的角度入手。 上界溢出1234567891011121314# 伪代码short int a;a = a + 1;# 对应的汇编movzx eax, word ptr [rbp - 0x1c]add eax, 1mov word ptr [rbp - 0x1c], axunsigned short int b;b = b + 1;# assembly codeadd word ptr [rbp - 0x1a], 1 上界溢出有两种情况，一种是 0x7fff + 1， 另一种是 0xffff + 1 0x7fff + 1因为计算机底层指令是不区分有符号和无符号的，数据都是以二进制形式存在 (编译器的层面才对有符号和无符号进行区分，产生不同的汇编指令)，所以 add 0x7fff, 1 == 0x8000，这种上界溢出对无符号整型就没有影响，但是在有符号短整型中，0x7fff 表示的是 32767，但是 0x8000 表示的是 -32768，用数学表达式来表示就是在有符号短整型中 32767+1 == -32768。 0xffff + 1第二种情况是 add 0xffff, 1，这种情况需要考虑的是第一个操作数。比如上面的有符号型加法的汇编代码是 add eax, 1，因为 eax=0xffff，所以 add eax, 1 == 0x10000，但是无符号的汇编代码是对内存进行加法运算 add word ptr [rbp - 0x1a], 1 == 0x0000。在有符号的加法中，虽然 eax 的结果为 0x10000，但是只把 ax=0x0000 的值储存到了内存中，从结果看和无符号是一样的。 再从数字层面看看这种溢出的结果，在有符号短整型中，0xffff==-1，-1 + 1 == 0，从有符号看这种计算没问题。但是在无符号短整型中，0xffff == 65535, 65535 + 1 == 0 下界溢出下届溢出的道理和上界溢出一样，在汇编代码中，只是把 add 替换成了 sub。一样也是有两种情况： 第一种是 sub 0x0000, 1 == 0xffff，对于有符号来说 0 - 1 == -1 没问题，但是对于无符号来说就成了 0 - 1 == 65535。 第二种是 sub 0x8000, 1 == 0x7fff，对于无符号来说是 32768 - 1 == 32767 是正确的，但是对于有符号来说就变成了 -32768 - 1 = 32767 从机器码的角度理解其本质不管数据的类型如何，在机器码层面，对于给定的容器空间，我们总可以按照理想情况去计算结果，然后根据容器大小对结果进行高位截断。此时容器中值的表示由上层编译器和数据的类型来决定。 总结产生溢出漏洞，主要来源于下面的两种可能： 未限制输入的范围 一个是没有限定初始输入的范围 另一个是计算过程中产生的溢出 错误的类型转换导致的溢出 参考资料 对溢出的分类：https://zh.wikipedia.org/wiki/%E6%BA%A2%E5%87%BA 整数溢出：https://ctf-wiki.github.io/ctf-wiki/pwn/linux/integeroverflow/intof/]]></content>
      <categories>
        <category>DSA</category>
      </categories>
      <tags>
        <tag>overflow</tag>
        <tag>opcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器]]></title>
    <url>%2F2019%2F07%2F28%2Fbloom-filter%2F</url>
    <content type="text"><![CDATA[定义在实际问题中，如果希望判断一个元素是否在一个集合中，我们可以使用的方法如下： 线性结构：array，vector，list 树：BST，trie hash 表：bitmap 随着元素数量的增多，我们需要的存储空间越来越大，同时检索的速度也越来越慢，而 bloom filter 正好能够解决这两个问题。 bloom filter 是一种用于判断给定元素是否在集合中的数据结构，它使用远小于 bitmap 的空间在牺牲一定正确性的情况下对元素作出判断。 简单来说，布隆过滤器 = 很长的二进制向量 + 一组随机 hash 函数 作用：用于检索一个元素是否在一个集合中 优点：空间效率和查询效率远超一般算法 缺点：有一定的错误率且元素删除很困难 原理构建 bloom filter实现一个超大的位数组和几个互不相关的哈希函数，假设位数组长度为 m，哈希函数个数为 k： 首先将位数组初始化，将每个位设置为 0 hash 函数能将元素映射为位数组上的一个点 当一个元素被加入集合中时，通过 k 个 hash 函数将这个元素映射为一个位数组中的 k 个点，将这些点置为 1 检索时，我们只需要判断查询元素对应的 k 个点是否为 1 就知道集合中有没有它了： 如果全为 1，认为集合中很可能存在这个元素，此处存在误判率 否则，认为不存在 误判率的产生原因：不同元素可能映射得到相同的点，这也是经典布隆过滤去不支持删除的原因 添加操作 将要添加的元素输入给 k 个哈希函数，得到对应于位数组上的 k 个位置 将这 k 个位置设为 1 查询操作 将要查询的元素输入给 k 个哈希函数，得到对应于位数组上的 k 个位置 如果 k 个位置有一个为 0，则肯定不在集合中 如果 k 个位置全部为 1，则可能在集合中 因此我们可以利用这一特性，对不在集合中的元素做出肯定的判断 分析关键：m 和 k 的选择 显然，很小的位数组很快就会装满，查询的任何值都会返回“可能存在“，起不到过滤的目的，误报率也就越高。因此，数组越长误报率越低 hash 函数越多过滤器中置 1 的速度就会越快，过滤器效率就会下降，误报率会增加 m 和 k 的选择取决于用户需要判断的元素数量 n 和期望的误判率 p ： 系统首先要计算需要的内存大小 m bits: 再由 m，n 得到哈希函数的个数： 想保持错误率低，布隆过滤器的空间使用率需为50% 总结优点相比其他数据结构，bloom filter 在时间上和空间上都有巨大优势： 插入和查询时间为常数 O(k) hash 函数之间相互没有关系，方便由硬件并行实现 数据结构不需要存储元素本身，适用于需要保密的场合 缺点 随着存入的元素数量增加，误算率随之增加，而如果元素较少，使用 hash table 足够了（我们可以反向利用这一点，bloom filter 能够判断某个元素是否一定不在） 一般情况下不能从该数据结构中删除元素，一个改进的方法是将位数组改为频率数组，每次增加点就对相应的位置增加 1，删除时减去 1 即可 启发 编码思路可以引入到算法设计中来 这里有 3-8 译码器的影子 参考资料 https://www.cnblogs.com/allensun/archive/2011/02/16/1956532.html https://zh.wikipedia.org/zh-hans/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8 https://blog.csdn.net/v_july_v/article/details/6685894]]></content>
      <categories>
        <category>DSA</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>bloom filter</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[约瑟夫环]]></title>
    <url>%2F2019%2F07%2F28%2F%E7%BA%A6%E7%91%9F%E5%A4%AB%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[01 题目编号为 1-N 的 N 个士兵围坐在一起形成一个圆圈，从编号为 1 的士兵开始依次报数（1，2，3…这样依次报），数到 m 的 士兵会被杀死出列，之后的士兵再从 1 开始报数。直到最后剩下一士兵，求这个士兵的编号。 02 问题分析假设只有三个人，把他们叫做A、B、C，三人围成一圈，从A开始报数，假设报 2 的人被杀掉。 首先A开始报数，他报1。侥幸逃过一劫。 然后轮到B报数，他报2。非常惨，他被杀了 C接着从1开始报数 接着轮到A报数，他报2。也被杀死了。 最终胜利者是C 通过上面的简单模拟我们可以发现这个问题的关键特点：从删除的地方重新开始计数 03 解题思路基础：使用数组进行标记算法：假设 n = 6, m = 3 用一个数组来存放 1，2，3 … n 这 n 个编号 遍历数组，对于被选中的编号，我们就做一个标记，例如编号 arr[2] = 3 被选中了，那么我们可以做一个标记，例如让 arr[2] = -1，来表示 arr[2] 存放的编号已经出局的了。 然后就按照这种方法，不停着遍历数组，不停着做标记，直到数组中只有一个元素是非 -1 的，这样，剩下的那个元素就是我们要找的元素了。 算法特点：思路简单，编码复杂 可以维护一个计数变量记录当前的位置 每次遍历到数组最后一个元素的时候，需要重新设置下标为 0 —— 可以用 mod 运算实现 遍历的时候需要判断该元素是否是 -1，并更新计数器 复杂度 时间：O(n*m) 空间：O(n) 优化：环形链表 —— 优化移除操作对容器的影响用环形链表来处理其实和数组处理的思路差不多，只是用链表来处理的时候，对于被选中的编号，不再是做标记，而是直接移除： 从链表移除一个元素的时间复杂度很低，为 O(1) 当然，上面数组的方法你也可以采用移除的方式，不过数组移除的时间复杂度为 O(n) 算法： 先创建一个环形链表来存放元素： 然后一边遍历链表一遍删除，直到链表只剩下一个节点 实现细节： 建立链表时，需要首尾相连 循环退出条件为 head 的 next 为 head 删除条件为计数到位，用 pre 和 cur 删除，往下遍历只需更新指针即可 复杂度： 时间：O(n*m) 空间：O(n) 优化：递归求解——找到前后序列的映射关系其实这道题还可以用递归来解决，递归是思路是每次我们删除了某一个士兵之后，我们就对这些士兵重新编号，然后我们的难点就是找出删除前和删除后士兵编号的映射关系。 我们定义递归函数 f(n，m) 的返回结果是存活士兵的编号： 递归基：显然当 n = 1 时，f(n, m) = 1 关系式：假如我们能够找出 f(n，m) 和 f(n-1，m) 之间的关系的话，我们就可以用递归的方式来解决了。 我们假设人员数为 n, 报数到 m 的人就自杀。则刚开始的编号为： 1… 1 ... m - 2, m - 1, m, m + 1, m + 2 ... n … 进行了一次删除之后，删除了编号为 m 的节点。删除之后，就只剩下 n - 1 个节点了，删除前和删除之后的编号转换关系为： 123456789101112131415删除前 --- 删除后… --- …m - 2 --- n - 2m - 1 --- n - 1m ---- 无(因为编号被删除了)m + 1 --- 1(因为下次就从这里报数了)m + 2 ---- 2… ---- … 新的环中只有 n - 1 个节点。且删除前编号为 m + 1, m + 2, m + 3 的节点成了删除后编号为 1， 2， 3 的节点。 通过上面你的分析可以看出：假设 old 为删除之前的节点编号， new 为删除了一个节点之后的编号，则 old 与 new 之间的关系为 （这个关系式有问题） 1old = (new + m - 1) % n + 1 这里将整个关系写出来： 12before: 1 , ..., m-1, m, m+1, ..., nafter : n-m+1, ..., n-1, , 1 , ..., n-m 需要注意的是，如果我们的序列从 1 开始计数，映射关系不好处理，但是如果从 0 开始计数，问题就变得简单了 这里在展开的时候有一个技巧，就是从删除的元素部分断开，然后按照 new 的序列从 0 开始排序即可 12old: m, m+1, m+2, ..., n-2, n-1, 0, 1, 2, ..., m-2new: 0, 1, 2, ..., n-m-2, n-m-1, n-m, n-m+1, n-m+2, ..., n-2 上面映射关系的表达式为 12345// 从 0 开始计数，分子部分的最大值为 n-1f(n, m) = [f(n-1, m) + m] % n// 从 1 开始计数，分子部分的最大值为 nf(n, m) = [f(n-1, m) + m + 1] % (n + 1) 最终输出结果时，如果题目是从 1 开始计数的，则结果 + 1 即可，如果从 0 开始计数，则直接输出 总结数学类的问题，可以尝试构造映射关系通过函数直接求解 参考资料 约瑟夫环：https://zhuanlan.zhihu.com/p/74436158，给的答案里面有陷阱 http://www.manongjc.com/article/80213.html https://segmentfault.com/a/1190000008230882 https://www.wjyyy.top/1966.html]]></content>
      <categories>
        <category>DSA</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>模拟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻牌游戏]]></title>
    <url>%2F2019%2F04%2F05%2Fpoker-problem%2F</url>
    <content type="text"><![CDATA[今天遇到一个有趣的问题。 游戏描述村长和小明进行一个翻牌游戏，规则如下： 共有 2019 张牌 二者轮流翻牌，每次翻 15 ～ 29 张 谁最后不能拿够 15 张牌则判定其失败 请给出小明的取胜策略。 问题分析游戏的中间过程存在一种状态：$m = 15 + 29 = 44$ 。在该种状态下，不管先手拿多少张牌，记为 $x$，后手总能拿 $y = 44 - x$ 张牌，且 $x, y$ 都满足拿牌规则的约束。 这个状态的意义在于，后手的人总能保证该轮游戏处于自己的控制之中，或者说游戏又回到了平衡的状态。 中间状态的增减不影响游戏的平衡性，但是会改变游戏的规模，基于此我们就可以对游戏策略进行分析，先来看最简单的情况，设总牌数为 $n$： 1. $n = k \times m, k = 1,2,..$即总牌数是中间状态的总倍数，这种情况下小明的必胜策略为：小明选择后手，且总是在该轮保证二者一共翻开 44 张牌，为了方便说明，我们设村长为 A，小明为 B，操作序列如下： 1A B A B A B ... A B A B a=0 我们对序列用括号进行描述： 1(A B) (A B) (A B) ... (A B) (A B) a=0 2. $n = k \times m + r, k = 1,2,.., r &lt; m$即总牌数在对中间状态去余后存在余数，那么原来对 $n$ 的策略变为对 $r$ 的策略。由于 $r$ 小于 $m$ ，若村长先手，则村长最少拿 15 张牌就能获胜（也必须拿 15 张牌），若小明先手则小明获胜，但是考虑到 1. 中提到的必胜策略是小明后手，因此只剩 r 张牌时小明不可能先手，因此小明在这种情况下选择后手必败（下面的序列中 $b$ 代表小明没有拿够 15 张牌。 1(A B) (A B) (A B) ... (A B) (A B) A b &lt; 15 那么小明能否选择先手呢？我们从操作序列来分析，下面是小明理想的操作序列（ $a$ 代表村长没有拿够 15 张牌），只要村长按照小明的意愿维护中间状态，小明就能获胜。但是，村长是不可能为小明维护中间状态的，因此这种理想情况是现实的。 1(B A) (B A) (B A) ... (B A) (B A) B a &lt; 15 观察序列可知，游戏过程中的先手后手是相对的，因此按下面的序列进行，小明又能够维护中间状态了： 1B (A B) (A B) (A ... B) (A B) (A B) a &lt; 15 去掉中间状态的干扰，问题变成了：给定 $r$ 张牌，小明先手，求小明的获胜策略。 在这种状态下，小明最少拿 15 张牌就能获胜（也必须拿 15 张牌）。 问题解答$2019 = 44 \times 45 + 39$，余数为 39。 若村长先手，每次取 $x$ 张，若小明在每轮中执行中间状态策略，每次取 $44 - x$ 张，则问题规模变为 39 张牌的问题，此时该村长不管怎么取，小明后手都无法完成游戏目标，小明失败，因此在村长先手的请款下小明不能直接维护中间状态！（过程中村长处于先手状态，如果其执行 2. 中小明的决策，则小明没有任何胜算，因此只能等待村长犯错！） 若小明先手，先任意取 25 ～ 29 张牌，之后变为村长先手，小明主动维护中间状态，最终会给村长留下 10 ～ 14 张牌，村长无法满足游戏目标，村长失败 总结 如果牌数与中间状态是倍数关系，小明后手可胜 如果存在余数，则小明先手可胜]]></content>
      <categories>
        <category>DSA</category>
      </categories>
      <tags>
        <tag>GameTheory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乔治的文献管理方案]]></title>
    <url>%2F2019%2F03%2F21%2Facdemic-manage%2F</url>
    <content type="text"><![CDATA[不得不承认，工具对人类的意义是颠覆性的：它不仅提升了工作效率，还改变了工作方式，甚至会影响你看待问题的角度。 今天利用此文总结 George 的文献管理方案，为大家的文献管理提供一点思路。 1. 准备实现这样的方案需要准备一些材料，他们包括： Zotero：一个开源免费文献管理软件（项目），直观的交互和还能忍受的界面是我选择它的理由，他也是整个文献管理方案的核心 Zotero for Chrome：让你一键完成 “右键 —— 另存 PDF 到本地 —— 导入 PDF 到管理软件 —— 构建文献索引” 这种日常文献检索操作 Google Scholar Extension：让你一键完成 “获取文献关键词 —— 打开 Google Scholar —— 键入关键词并检索 —— 获取文献 PDF”这样的日常文献检索操作 OneDrive：微软出品的云同步盘，跨平台的核心，当然你可以用其他云同步盘来替代 2. 方案整个方案分为三个部分： 使用 Zotero 高效管理文献 在不同的物理设备上同步 Zotero 数据库 在同一主机不同的逻辑设备上同步 Zotero 数据库 3. 使用 Zotero 高效管理文献首先解决 Zotero 的使用问题​​使用 Zotero 管理文献很简单： 首先打开软件 然后将本地文献文件拖入软件即可 之后 Zotero 会自动基于在线数据库为你的文献创建索引（索引包括但不限于：标题，作者，摘要、期刊、……） 你也可以为你的文献创建笔记，添加标签，使用文件系统对文献进行分类。 详细的内容请参阅 Zotero 的官网：https://www.zotero.org/ 然后解决高效使用 Zotero 的问题如果你准备好了前文介绍的材料（安装并调试好了相关插件），这里我只需要为你描述一下整个场景就可以了： 你在知乎看到一篇有关分布系统领域经典文献的综述回答，我需要获取 GFS、MapReduce、BigTable 的文献资料 在回答中左键选取需要的关键词，之后单击浏览器上的谷歌学术快捷按钮，此时在网络状态良好的情况下，你会得到一个相关文献列表 选择最相关的那篇文献，点击 PDF 按钮跳转到相关页面，如果你运气好的话，你就看到了你需要的文献 在 PDF 文献页面点击 Save to Zotero 按钮，选择你希望存储的目录，等待 Zetero 工作完成即可（请先启动本地 Zotero），此时你在 Zotero 中可以看到文献已经加入你的文献学习大礼包了 ctrl + w 关闭 PDF 页面，结束战斗或者跳转步骤 2 😄 如果你只在一个设备（工作笔记本或者台式机）上进行文献管理，OK，今天的安利结束，祝您科研愉快。 如果你有跨平台、多设备的需求，那么可以再往下看看 ⬇️ 4. 在不同的物理设备上同步 Zotero 数据库首先我们来看看乔治的文献管理需求： 需要在 Windows 笔记本上查阅和标注文献 需要在工作主机的 Win / Linux 双系统 PC 上查阅和标注文献 偶尔需要在 Pad 上阅读文献 希望能够将文献数据备份到云端 乔治内心 OS：研究没啥成果，要求还蛮多的嘛 我们想让一个软件能够跨平台、多设备的使用，那么这个软件应该要具备两个特性： 数据文件相对软件本身独立 数据文件对不同客户端状态的影响具有一致性 嘿嘿，这两点 Zotero 都能满足（其他的也没用过 😺）： Zotero 作为一个开源的、提供 Linux 系统软件的项目，其数据文件的设计应该是考虑到跨平台需要的（Win 和 Linux 下文件组织方式一致），也就是说你把 Win 环境下的数据文件 COPY 到 Linux 环境下，OK，Zotero 也能够正确恢复，二者保证了 Zotero 软件状态的一致性 那么，进一步的，Zotero 是如何管理我的文献的？ 你的文献记录在 Zotero 根目录下，使用 sqllite 数据库存储。你的文献数据存储在 Zotero 根目录下的 Storage 文件夹中，每个文献用一个含有神秘字符串的文件夹进行归档，文件夹中包括： .PDF：你的文献 cache：文献的索引 info：文件的信息 因此，实现跨平台和多设备同步访问只需要——将 usr 目录下的 Zotero 文件夹加入同步盘的同步大礼包！然后在不同设备上进行同步即可。 5. 在同一主机不同的逻辑设备上同步 Zotero 数据库这句话是什么意思呢，多亏乔治爱折腾的坏毛病，乔治将它的 PC 装了双系统，将 SSD 分区来做系统盘，将硬盘作为共享盘（类似两个进程之间的共享内存）。乔治希望在两个系统之间共享同样的文献数据，同时不希望通过 Onedrive 将数据通过网络在本地复制两份（Linux 上也没有客户端）。 怎么办？只需要将 Zetoro 的数据重定位到共享硬盘就行了。 这里还存在另一个问题，每次在 Linux 环境下使用的时候都要先挂载共享硬盘，乔治的解决方案是写一个脚本开机自动挂载需要的分区即可。 OK，至此乔治这变态的文献管理需求就解决了，如果大家关于文献管理或者科研有什么心得或者建议欢迎通过邮件和乔治交流哦。]]></content>
      <categories>
        <category>效率工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[下载]]></title>
    <url>%2F2019%2F03%2F18%2Fdownload%2F</url>
    <content type="text"><![CDATA[首先，乔治在这里感谢迅雷、百度网盘、115 网盘等国产老牌下载工具，谢谢你们给我了机会去研究“下载”，它包括但不限于： HTTP / HTTPS FTP / SFTP BT 磁力链 电驴链接 迅雷链接 HTTP / HTTPSHTTP 是基于 C/S 通信模型的文件传输协议。协议全称为超文本传输协议，主要用于在互联网上传输相对较小的由超文本语言描述的文本文件和媒体文件，这些文件可以直接被我们的客户端（浏览器）解析和表示。正如其名，它可以用来传输资源。当我们借助浏览器（协议的客户端）去请求特定的网页时，其实就是在下载特定的文件。具体来说，就是采用 HTTP 请求的方式从服务器指定的文件路径请求相应的资源文件，常用的请求链接如下： 1http://sqdownd.onlinedown.net/down/bitcomet_setup150.exe 类比我们在本地主机上访问文件的过程，我们可以将这个过程想象为从远端主机访问文件，二者的区别在于本地访问使用本地文件路径，HTTP 请求使用主机地址+通信端口（80）+远程文件路径由于 HTTP 协议并非为文件（大文件）传输而设计，因此无法控制文件的传输过程和传输效率，会出现三种常见情况： 无法断点续传 传输速度慢 并发能力弱 安全性低 FTP / SFTPFTP 是基于 C/S 通信模型的文件传输协议（正规军）。和 HTTP 一样，FTP 也是将文件放到服务器上，由服务器传输到不同的主机。FTP 在 HTTP 的基础上增加了认证和传输控制过程，有的资源站出于方便的目的会采用 FTP 的 Anonymous 方法来跳过验证步骤。FTP 的常用链接如下： 12ftp://ftpserver:port/resourceNameftp://4:4@dx.dl1234.com:8006/阿甘正传BD双语双字[电影天堂www.dy2018.com].mkv BTBT 是基于 P2P 通信模型的文件共享协议。P2P 模型所具有的特点为： 无中心的对等网络，每个节点既是客户端，也是服务器 文件采用分块存储，方便并发传输 持有相同资源的用户越多，下载文件的速度越快 BT 下载原理： 从互联网等渠道获取种子文件 客户端解析种子文件获得 Tracker 地址，连接 Tracker 服务器 Tracker 服务器响应情况，告知其他持有资源的节点的位置（IP），客户端连接其他节点 客户端根据种子文件和对端交换缺失的文件块，每下载一块就利用种子文件校验其正确性（HASH） BT 协议的特点： 速度快 保护隐私 减轻服务器压力 存在版权问题 磁力链接旧的 BT 协议虽然是去中心化的，但是系统中存在 Tracker 这样的关键点，因此引入 DHT 技术来解决这个问题。DHT 全称分布式哈希表，是一种分布式检索和存储技术，使用 DHT 的 BT 协议可以在没有 Tracker 的参与时使用 DHT 寻址的方法获得其他节点的位置信息。 磁力链接是目标文件的 HASH，使用该 HASH 在 DHT 中搜索需要的种子文件，文件完整性更好，是一种基于文件的寻址方式。 电驴 2000, metaCachine 公司开发了 eDonkey2000，一个跨平台的闭源 P2P 共享软件，由三部分组成： eDonkey 网络：一个 P2P 的文件共享网络，后升级为 DHT 网络 eDonkey 软件：能够连接 eDonkey 网络的节点 eDonkey 链接：用于指示网络中文件的链接，是文件内容的 HASH 1ed2k://|file|%E6%B2%B3%E7%95%94.720p.BD%E4%B8%AD%E5%AD%97[%E6%9C%80%E6%96%B0%E7%94%B5%E5%BD%B1www.66ys.tv].mp4|1284891202|5052C583C3EC5A5BC626F45847CB0340|h=5QE3DFPQBGH5JXPBSZ4P5IHVAUS3HWPY|/ 电骡虽然 eDonkey2000 倒下了，但是后起之秀其实在它刚诞生1年就已经出现了，那就是 eMule，因为 eMule 在中文是骡子的意思，所以中文正式翻译是电骡。它是一款开源软件，它可以看作是是商业软件 eDonkey 的同人作品它除了支持 eDonkey 的网络和 eD2k 协议之外，还新增了很多不一样的功能，特别是 KAD 节点（类似 BT 的 HDT 网络）的支持，让 eD2k 彻底成为了完全的 P2P 网络eDonkey 电驴所使用的 eDonkey 网络，虽然也是基于 P2P 共享，但是仍然离不开中央服务器。一群人使用 eDonkey 电驴进行文件共享，必须要有中央服务器来作为通讯中心，为用户提供查找文件等服务。而 KAD 则是更彻底的 P2P，只需要用户作为节点，用户之间能够直接连接，完全脱离了中央服务器。这种方式其实也因此经受住了版权商的压力而巍然不动，因为用户之间共享盗版可以推脱给用户的自发行为 easyMule上文提到国内的 eMule 发展的不温不火，于是上海维西（VeryCD）公司看到了商机，因为 eMule 是开源的，所以2007年起开始将 eMule 的代码拿来开发，改名叫 easyMule。easyMule 第一版为 eMule Mod，修改自官方 eMule，因而遵循 GNU GPL v2 协议，开放源代码；第二版声称完全自主开发而闭源，但其是否真正完全自主开发尚存在争议。VeryCD 公司宣传称 easyMule 为“电驴”，此名称存在较大争议，因为明显跟 eDonkey 电驴容易起混淆在起初，VeryCD 电驴还保留着 eMule 的大量功能，但到了后来，VeryCD 电驴将最精髓的 KAD 资源搜索等功能重重阉割（KAD 可以搜索全球e Mule 共享的资源，VeryCD 电驴后来只能搜索自己官网的资源），而且宣传上更是将自己当做是正牌电驴。在好长一段时间内，搜索引擎搜“电驴”、“电骡”、“eMule”，排名首页的一度是 VeryCD官网而非 eMule-Project 官网。VeryCD 官网通过把 eD2k 资源进行了整合，建立起了 eD2k 分享平台，一度成为了中国大陆浏览量最大的资源分享网站之一 其他协议迅雷链接、快车链接、旋风链接都不是协议，只是单纯对字符串做一些编码操作，好让别的下载软件识别不了，而自家的软件因为知道解码规则所以可以。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>download</tag>
        <tag>bt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker]]></title>
    <url>%2F2019%2F03%2F12%2Fdocker%2F</url>
    <content type="text"><![CDATA[容器技术容器技术又称为容器虚拟化，是虚拟化技术的一种 虚拟化技术有硬件虚拟化、半虚拟化和操作系统虚拟化，Docker属于操作系统虚拟化，其相较于其他主流技术更轻量 虚拟化技术总结： 硬件虚拟化 软件虚拟化 应用虚拟化 Wine 平台虚拟化：虚拟机 操作系统虚拟化 容器技术 Docker 容器技术本身借鉴了工业运输的经验发展而来，《经济学人》这样评价工业运输领域的集装箱：没有集装箱，就没有全球化。 物流领域的集装箱实现了货物的标准化，以此建立了全球范围的物流系统。 软件行业的容器技术也在尝试打造一套标准化的软件构建、分发流程，以降低运维成本，提高软件的安全与稳定。 容器技术的发展如果说工业上的集装箱是从一个箱子开始的，那么软件行业的容器就是从文件系统的隔离开始的。 1979，最早的容器技术大概是 chroot，它是 UNIX 系统上的一个系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置，让该进程只能访问该目录。 2000，Derrick T. Woworth 为 FreeBSD 引入了 Jails，它可以为文件系统、用户、网络等的隔离增加进程沙盒功能 2001，Linux VServer 出现，其中的虚拟系统叫做 VPS 2001，Virtuozzo 2004，Solaris Containers 2005，OpenVZ 2006，Process Containers，后来改名为 Control Groups（CGroups） 2008，LXC 2011，Warden 2013，Lmctfy 2013，Docker 2014，Rcket 容器技术的核心原理容器技术的核心是 Cgroups（资源控制） 和 Linux namespace（资源隔离）。本质是宿主机上的进程 容器与 DockerDocker 的核心可能是： 对分层镜像的创新应用 统一了应用的打包、分发和部署方式 Docker 的创新不一定要依赖容器技术，容器成就了 Docker，Docker 促进容器技术的发展 如何理解 Docker Docker 是一个开源的应用容器引擎，开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到主流的平台上，实现虚拟化。 Docker 是一个重新定义了程序开发、测试、交付和部署的开放平台，Docker 就是集装箱，帮你把软件运输到不同的平台，并迅速部署 What is DockerDocker 是一种操作系统层面的虚拟化技术，由于其隔离的进程独立于宿主以及其他的隔离的进程，因此也称其为容器。 传统虚拟机技术是：虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。 而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 Why Docker相比于传统的虚拟化方式，Docker 具有众多优势： 对系统资源利用率更高：应用执行速度、内存损耗、文件IO 更快的启动时间：毫秒级启动时间 一致的运行环境：开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未能够在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题 持续交付和部署：对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署 更轻松的迁移：Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的，因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况 更轻松的维护和扩展：Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单 对虚拟机技术的比较 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为 MB 一般为 GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 Docker Base ConceptDocker 包含了三个基本概念：image、container、registry Docker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，Docker 会从镜像仓库下载该镜像 Docker ImageDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变 文件系统：Docker 镜像（Image），就相当于是一个 root 文件系统（挂在于 内核态 中供 用户态 使用） 分层存储：Docker 使用 Union FS 技术，使用分层存储技术优化 Image 的体积和系统的构建 Docker Container镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 容器的实质是进程，属于自己独立的命名空间，运行于隔离环境中 每一个容器运行时，以镜像为基础层，在其上创建一个当前容器的存储层，为容器运行时的读写做准备，存储层的信息随容器的删除而丢失 Docker 中文件的写入应该使用 数据卷（Volume）或绑定宿主目录，直接对宿主进行读写，性能和稳定性更高 数据卷独立于容器，容器删除后数据不会丢失 Docker RegistryDocker Registry 用于存储、分发构建好的镜像，方便其他宿主使用。 一个 Registry 中可以包含多个仓库 Repository，每个仓库包含多个标签 Tag，每个标签对应一个镜像。通常，一个仓库会包含同一软件的不同版本镜像，而标签常用于对应软件的各个版本 &lt;repository&gt;:&lt;tag&gt; ，如果不给出标签，将以 latest 作为默认标签 仓库名常以两段式路径形式出现，如 super/nginx-proxy ，前者往往表示多用户环境下的用户名，后者往往是对应的软件名 公开服务Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像 Docker Hub：https://hub.docker.com/ coreOS：https://coreos.com/ quay.io：https://quay.io/repository/ Google Container Registry：https://cloud.google.com/container-registry/ 国内的一些云服务商提供针对 Docker Hub 的镜像服务 Registry Mirror ，这些镜像服务被称为加速器 DaoCloud 加速器：https://www.daocloud.io/mirror#accelerator-doc 阿里云加速器：https://cr.console.aliyun.com/cn-hangzhou/new 国内也有一些云服务商提供类似 Docker Hub 的公开服务，比如时速云镜像仓库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库 等 私有服务除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry镜像，可以直接使用做为私有 Registry 服务 Installing Docker安装工作Docker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。Docker CE 分为 stable, test, 和 nightly 三个更新频道。每六个月发布一个 stable 版本 (18.09, 19.03, 19.09…) 官方网站上有 Docker 各种环境下的 安装指南，请参阅安装指南 关键工作启动 Docker 12sudo systemctl enable dockersudo systemctl start docker 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组 12sudo groupadd dockersudo usermod -aG docker $USER 测试安装 1docker run hello-world 配置加速器 1234567891011$ vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125;$ sudo systemctl daemon-reload$ sudo systemctl restart docker 验证加速器 1docker info Using Image从仓库获取镜像下载 从 Docker 镜像仓库获取镜像： Docker 镜像仓库地址：地址的格式一般是 &lt;域名/IP&gt;[:端口号]。默认地址是 Docker Hub 仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt;。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像 12345678910docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]$ docker pull ubuntu:18.04bf5d46315322: Pull complete9f13e0ac480c: Pull completee8988b5b3097: Pull complete40af181810e7: Pull completee6f7c7e5c03e: Pull completeDigest: sha256:147913621d9cdea08853f6ba9116c2e27a3ceffecf3b492983ae97c3d643fbbeStatus: Downloaded newer image for ubuntu:18. 注意：从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 ID 的前 12 位。并且下载结束后，给出该镜像完整的 sha256 的摘要，以确保下载一致性 运行 有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器 1$ docker run -it --rm ubuntu:18.04 bash -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash 管理本地镜像列出镜像 要想列出已经下载下来的镜像，可以使用 docker image ls 命令 12345678$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MBmongo 3.2 fe9198c04d62 5 days ago 342 MB&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MBubuntu 18.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MB 列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间，镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到 ubuntu:18.04 和 ubuntu:latest 拥有相同的 ID，因为它们对应的是同一个镜像 镜像体积 如果仔细观察，会注意到，这里标识的所占用空间和在 Docker Hub 上看到的镜像大小不同。比如，ubuntu:18.04 镜像大小，在这里是 127 MB，但是在 Docker Hub 显示的却是 50 MB。这是因为 Docker Hub 中显示的体积是压缩后的体积。在镜像下载和上传过程中镜像是保持着压缩状态的，因此 Docker Hub 所显示的大小是网络传输中更关心的流量大小。而 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小 另外一个需要注意的问题是，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多 可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间 1234567$ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 24 0 1.992GB 1.992GB (100%)Containers 1 0 62.82MB 62.82MB (100%)Local Volumes 9 0 652.2MB 652.2MB (100%)Build Cache 0B 0B 虚悬镜像 一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 &lt;none&gt; ，一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除 1$ docker image prune 中间层镜像 为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数 1$ docker image ls -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除 列出部分镜像 不加任何参数的情况下，docker image ls 会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情 根据仓库名列出镜像 1234$ docker image ls ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MB 列出特定的某个镜像，也就是说指定仓库名和标签 123$ docker image ls ubuntu:18.04REPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 f753707788c5 4 weeks ago 127 MB 删除本地镜像 如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为： 1$ docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] &lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要 ，我们可以用镜像的完整 ID，也称为 长 ID，来删除镜像。使用脚本的时候可能会用长 ID，但是人工输入就太累了，所以更多的时候是用 短 ID 来删除镜像。docker image ls 默认列出的就已经是短 ID 了，一般取前3个字符以上，只要足够区分于别的镜像就可以了 删除行为分为两类，一类是 Untagged，另一类是 Deleted。我们之前介绍过，镜像的唯一标识是其 ID 和摘要，而一个镜像可以有多个标签，因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 Untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 Delete 行为就不会发生。所以并非所有的 docker image rm 都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。 当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变动非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的源 除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像 镜像基本原理镜像是容器的基础，每次执行 docker run 的时候都会指定哪个镜像作为容器运行的基础。在之前的例子中，我们所使用的都是来自于 Docker Hub 的镜像。直接使用这些镜像是可以满足一定的需求，而当这些镜像无法直接满足需求时，我们就需要定制这些镜像。接下来的几节就将讲解如何定制镜像 回顾一下之前我们学到的知识，镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层 当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。 1234567docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]$ docker commit \ --author &quot;Tao Wang &lt;twang2218@gmail.com&gt;&quot; \ --message &quot;修改了默认网页&quot; \ webserver \ nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用 回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿 ×使用 Dockerfile 定制镜像Using Container简单的说，容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用 启动容器启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。因为 Docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器 新建并启动 1$ docker run ubuntu:18.04 /bin/echo &apos;Hello world&apos; 当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 启动已终止容器 可以利用 docker container start 命令，直接将一个已经终止的容器启动运行 注意：容器的核心为所执行的应用程序，所需要的资源都是应用程序运行所必需的。除此之外，并没有其它的资源。可以在伪终端中利用 ps 或 top 来查看进程信息 1234root@ba267838cc1b:/# ps PID TTY TIME CMD 1 ? 00:00:00 bash 11 ? 00:00:00 ps 可见，容器中仅运行了指定的 bash 应用。这种特点使得 Docker 对资源的利用率极高，是货真价实的轻量级虚拟化 后台运行更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现 12$ docker run -d ubuntu:18.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a 要获取容器的输出信息，可以通过 docker container logs 命令 12345$ docker container logs [container ID or NAMES]hello worldhello worldhello world. . . 终止容器可以使用 docker container stop 来终止一个运行中的容器，此外，当 Docker 容器中指定的应用终结时，容器也自动终止 终止状态的容器可以用 docker container ls -a 命令看到 1234docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba267838cc1b ubuntu:18.04 "/bin/bash" 30 minutes ago Exited (0) About a minute ago trusting_newton98e5efa7d997 training/webapp:latest "python app.py" About an hour ago Exited (0) 34 minutes ago backstabbing_pike 处于终止状态的容器，可以通过 docker container start 命令来重新启动 此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它 进入容器某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令 attach 命令 123456789$ docker run -dit ubuntu243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES243c32535da7 ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds nostalgic_hypatia$ docker attach 243croot@243c32535da7:/# 注意： 如果从这个 stdin 中 exit，会导致容器的停止 exec 命令 docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。 只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。 当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符 12345678910111213141516$ docker run -dit ubuntu69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69d137adef7a ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds zealous_swirles$ docker exec -i 69d1 bashlsbinbootdev...$ docker exec -it 69d1 bashroot@69d137adef7a:/# 注意：如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因 导入和导出容器如果要导出本地某个容器，可以使用 docker export 命令，这样将导出容器快照到本地文件 1234$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:18.04 "/bin/bash" 36 hours ago Exited (0) 21 hours ago test$ docker export 7691a814370e &gt; ubuntu.tar 可以使用 docker import 从容器快照文件中再导入为镜像 1234$ cat ubuntu.tar | docker import - test/ubuntu:v1.0$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外，也可以通过指定 URL 或者某个目录来导入 1$ docker import http://example.com/exampleimage.tgz example/imagerepo 注意：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息 删除容器可以使用 docker container rm 来删除一个处于终止状态的容器 12$ docker container rm trusting_newtontrusting_newton 如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器 用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器 1$ docker container prune Using Repository仓库（Repository）是集中存放镜像的地方，注册服务器（Registry）是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。 从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址，ubuntu 是仓库名 Docker Hub注册 —》登录 —》拉取 / 推送 登录 可以通过执行 docker login 命令交互式的输入用户名及密码来完成在命令行界面登录 Docker Hub，你可以通过 docker logout 退出登录 拉取 你可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull命令来将它下载到本地 1234567$ docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 465 [OK]tianon/centos CentOS 5 and 6, created using rinse instea... 28blalor/centos Bare-bones base CentOS 6.5 image 6 [OK]saltstack/centos-6-minimal 6 [OK]tutum/centos-6.4 DEPRECATED. Use tutum/centos:6.4 instead. ... 5 [OK] 可以看到返回了很多包含关键字的镜像，其中包括镜像名字、描述、收藏数（表示该镜像的受关注程度）、是否官方创建、是否自动创建。 官方的镜像说明是官方项目组创建和维护的，automated 资源允许用户验证镜像的来源和内容。 根据是否是官方提供，可将镜像资源分为两类。 一种是类似 centos 这样的镜像，被称为基础镜像或根镜像。这些基础镜像由 Docker 公司创建、验证、支持、提供。这样的镜像往往使用单个单词作为名字。 还有一种类型，比如 tianon/centos 镜像，它是由 Docker 的用户创建并维护的，往往带有用户名称前缀。可以通过前缀 username/ 来指定使用某个用户提供的镜像，比如 tianon 用户。 另外，在查找的时候通过 --filter=stars=N 参数可以指定仅显示收藏数量为 N 以上的镜像 下载 123456$ docker pull centosPulling repository centos0b443ba03958: Download complete539c0211cd76: Download complete511136ea3c5a: Download complete7064731afe90: Download complete 推送 用户也可以在登录后通过 docker push 命令来将自己的镜像推送到 Docker Hub。 1234567891011121314$ docker tag ubuntu:18.04 username/ubuntu:18.04$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 275d79972a86 6 days ago 94.6MBusername/ubuntu 18.04 275d79972a86 6 days ago 94.6MB$ docker push username/ubuntu:18.04$ docker search usernameNAME DESCRIPTION STARS OFFICIAL AUTOMATEDusername/ubuntu 自动创建 自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。 有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。 而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub 或 BitBucket）上的项目，一旦项目发生新的提交或者创建新的标签（tag），Docker Hub 会自动构建镜像并推送到 Docker Hub 中 要配置自动创建，包括如下的步骤： 创建并登录 Docker Hub，以及目标网站； 在目标网站中连接帐户到 Docker Hub； 在 Docker Hub 中 配置一个自动创建； 选取一个目标网站中的项目（需要含 Dockerfile）和分支； 指定 Dockerfile 的位置，并提交创建。 之后，可以在 Docker Hub 的 自动创建页面 中跟踪每次创建的状态 ×私有仓库Docker 数据管理这一章介绍如何在 Docker 内部以及容器之间管理数据，在容器中管理数据主要有两种方式： 数据卷（Volumes） 挂载主机目录 (Bind mounts) 数据卷数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的 数据卷 创建一个数据卷 1$ docker volume create my-vol 查看所有的数据卷 123$ docker volume lslocal my-vol 查看指定数据卷的信息 1$ docker volume inspect my-vol 启动一个挂载数据卷的容器 在用 docker run 命令的时候，使用 --mount 标记来将 数据卷 挂载到容器里。在一次 docker run 中可以挂载多个 数据卷 12345$ docker run -d -P --name web \ # -v my-vol:/wepapp \ --mount source=my-vol,target=/webapp \ training/webapp \ python app.py 查看数据卷的具体信息 在主机里使用以下命令可以查看 web 容器的信息 1$ docker inspect web 删除数据卷 1$ docker volume rm my-vol 数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，Docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令 无主的数据卷可能会占据很多空间，要清理请使用以下命令 1$ docker volume prune 挂载主机目录挂载一个主机目录作为数据卷 使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去 123456$ docker run -d -P \ --name web \ # -v /src/webapp:/opt/webapp \ --mount type=bind,source=/src/webapp,target=/opt/webapp \ training/webapp \ python app.py 上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 -v 参数时如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 --mount 参数时如果本地目录不存在，Docker 会报错 Docker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读 123456$ docker run -d -P \ --name web \ # -v /src/webapp:/opt/webapp:ro \ --mount type=bind,source=/src/webapp,target=/opt/webapp,readonly \ training/webapp \ python app.py 挂载一个本地主机文件作为数据卷 --mount 标记也可以从主机挂载单个文件到容器中 123456789$ docker run --rm -it \ # -v $HOME/.bash_history:/root/.bash_history \ --mount type=bind,source=$HOME/.bash_history,target=/root/.bash_history \ ubuntu:18.04 \ bashroot@2affd44b4667:/# history1 ls2 diskutil list 这样就可以记录在容器输入过的命令了 Docker 使用网络Docker 允许通过外部访问容器或容器互联的方式来提供网络服务 外部访问容器容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p参数来指定端口映射 随机映射 当使用 -P 标记时，Docker 会随机映射一个 49000~49900 的端口到内部容器开放的网络端口 使用 docker container ls 可以看到，本地主机的 49155 被映射到了容器的 5000 端口。此时访问本机的 49155 端口即可访问容器内 web 应用提供的界面 1$ docker run -d -P training/webapp python app.py 指定地址的指定端口 -p 则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。可以使用 ip:hostPort:containerPort 格式指定映射使用一个特定地址，比如 localhost 地址 127.0.0.1 1$ docker run -d -p 5000:5000 training/webapp python app.py 指定地址的任意端口 1$ docker run -d -p 127.0.0.1::5000 training/webapp python app.py 容器有自己的内部网络和 ip 地址（使用 docker inspect 可以获取所有的变量，Docker 还可以有一个可变的网络配置。） -p 标记可以多次使用来绑定多个端口 1$ docker run -d -p 5000:5000 -p 3000:80 training/webapp python app.py 容器互联如果你之前有 Docker 使用经验，你可能已经习惯了使用 --link 参数来使容器互联。随着 Docker 网络的完善，强烈建议大家将容器加入自定义的 Docker 网络来连接多个容器，而不是使用 --link 参数 新建网络 1$ docker network create -d bridge my-net -d 参数指定 Docker 网络类型，有 bridge overlay。其中 overlay 网络类型用于 Swarm mode 连接容器 12$ docker run -it --rm --name busybox1 --network my-net busybox sh$ docker run -it --rm --name busybox2 --network my-net busybox sh 连接测试 下面通过 ping 来证明 busybox1 容器和 busybox2 容器建立了互联关系 1234/ # ping busybox2PING busybox2 (172.19.0.3): 56 data bytes64 bytes from 172.19.0.3: seq=0 ttl=64 time=0.072 ms64 bytes from 172.19.0.3: seq=1 ttl=64 time=0.118 ms 注意：如果你有多个容器之间需要互相连接，推荐使用 Docker Compose 配置容器的 DNS如何自定义配置容器的主机名和 DNS 呢？秘诀就是 Docker 利用虚拟文件来挂载容器的 3 个相关配置文件 在容器中使用 mount 命令可以看到挂载信息： 1234$ mount/dev/disk/by-uuid/1fec...ebdf on /etc/hostname type ext4 .../dev/disk/by-uuid/1fec...ebdf on /etc/hosts type ext4 ...tmpfs on /etc/resolv.conf type tmpfs ... 这种机制可以让宿主主机 DNS 信息发生更新后，所有 Docker 容器的 DNS 配置通过 /etc/resolv.conf 文件立刻得到更新 配置全部容器的 DNS ，也可以在 /etc/docker/daemon.json 文件中增加以下内容来设置 123456&#123; "dns" : [ "114.114.114.114", "8.8.8.8" ]&#125; 这样每次启动的容器 DNS 自动配置为 114.114.114.114 和 8.8.8.8。使用以下命令来证明其已经生效 1234$ docker run -it --rm ubuntu:18.04 cat etc/resolv.confnameserver 114.114.114.114nameserver 8.8.8.8 如果用户想要手动指定容器的配置，可以在使用 docker run 命令启动容器时加入如下参数： -h HOSTNAME 或者 --hostname=HOSTNAME 设定容器的主机名，它会被写到容器内的 /etc/hostname 和 /etc/hosts。但它在容器外部看不到，既不会在 docker container ls 中显示，也不会在其他的容器的 /etc/hosts 看到。 --dns=IP_ADDRESS 添加 DNS 服务器到容器的 /etc/resolv.conf 中，让容器用这个服务器来解析所有不在 /etc/hosts 中的主机名。 1--dns-search=DOMAIN` 设定容器的搜索域，当设定搜索域为 `.example.com` 时，在搜索一个名为 host 的主机时，DNS 不仅搜索 host，还会搜索 `host.example.com 注意：如果在容器启动时没有指定最后两个参数，Docker 会默认用主机上的 /etc/resolv.conf 来配置容器 ×高级网络配置Docker 的互联机制： 当 Docker 启动时，会自动在主机上创建一个 docker0 虚拟网桥，实际上是 Linux 的一个 bridge，可以理解为一个软件交换机。它会在挂载到它的网口之间进行转发 同时，Docker 随机分配一个本地未占用的私有网段（在 RFC1918 中定义）中的一个地址给 docker0 接口。比如典型的 172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段（172.17.0.0/16）的地址 当创建一个 Docker 容器的时候，同时会创建了一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0；另一端在本地并被挂载到 docker0 网桥，名称以 veth 开头（例如 vethAQI2QT）。通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。Docker 就创建了在主机和所有容器之间一个虚拟共享网络 Docker ComposeCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig 通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project） Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理 使用 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元 最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。下面我们用 Python 来建立一个能够记录页面访问次数的 web 网站 web 应用 —》Dockerfile —》docker-compose.yml —》运行 compose 编写 Dockerfile 12345FROM python:3.6-alpineADD . /codeWORKDIR /codeRUN pip install redis flaskCMD ["python", "app.py"] 编写 docker-compose.yml 编写 docker-compose.yml 文件，这个是 Compose 使用的主模板文件 12345678910version: '3'services: web: build: . ports: - "5000:5000" redis: image: "redis:alpine" 运行 1$ docker-compose up ×命令说明1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。 --x-networking 使用 Docker 的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出 ×compose 模板文件Docker MachineDocker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker 环境。Docker Machine 支持多种后端驱动，包括虚拟机、本地主机和云平台等。 Docker SwarmSwarm 的基本概念Docker Swarm 是 Docker 官方三剑客项目之一，提供 Docker 容器集群服务，是 Docker 官方对容器云生态进行支持的核心方案。使用它，用户可以将多个 Docker 主机封装为单个大型的虚拟 Docker 主机，快速打造一套容器云平台 Swarm mode 内置 kv 存储功能，提供了众多的新特性，比如：具有容错能力的去中心化设计、内置服务发现、负载均衡、路由网格、动态伸缩、滚动更新、安全传输等。使得 Docker 原生的 Swarm 集群具备与 Mesos、Kubernetes 竞争的实力 Swarm 是使用 SwarmKit 构建的 Docker 引擎内置（原生）的集群管理和编排工具 使用 Swarm 集群之前需要了解以下几个概念： 节点 运行 Docker 的主机可以主动初始化一个 Swarm 集群或者加入一个已存在的 Swarm集群，这样这个运行 Docker 的主机就成为一个 Swarm 集群的节点 (node) 节点分为管理 (manager) 节点和工作 (worker) 节点。管理节点用于 Swarm 集群的管理，docker swarm 命令基本只能在管理节点执行（节点退出集群命令 docker swarm leave 可以在工作节点执行）。一个 Swarm 集群可以有多个管理节点，但只有一个管理节点可以成为 leader，leader 通过 raft 协议实现 工作节点是任务执行节点，管理节点将服务 (service) 下发至工作节点执行。管理节点默认也作为工作节点。你也可以通过配置让服务只运行在管理节点 服务和任务 任务 （Task）是 Swarm 中的最小的调度单位，目前来说就是一个单一的容器 服务 （Services） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式： replicated services 按照一定规则在各个工作节点上运行指定个数的任务。 global services 每个工作节点上运行一个任务 两种模式通过 docker service create 的 --mode 参数指定 ×Swarm 的使用Docker 安全特性评估 Docker 的安全性时，主要考虑三个方面: 由内核的命名空间和控制组机制提供的容器内在安全 Docker 程序（特别是服务端）本身的抗攻击性 内核安全性的加强机制对容器安全性的影响 内核命名空间Docker 容器和 LXC 容器很相似，所提供的安全特性也差不多。当用 docker run 启动一个容器时，在后台 Docker 为容器创建了一个独立的命名空间和控制组集合 命名空间提供了最基础也是最直接的隔离，在容器中运行的进程不会被运行在主机上的进程和其它容器发现和作用 每个容器都有自己独有的网络栈，意味着它们不能访问其他容器的 sockets 或接口。不过，如果主机系统上做了相应的设置，容器可以像跟主机交互一样的和其他容器交互。当指定公共端口或使用 links 来连接 2 个容器时，容器就可以相互通信了（可以根据配置来限制通信的策略） 控制组控制组是 Linux 容器机制的另外一个关键组件，负责实现资源的审计和限制 它提供了很多有用的特性；以及确保各个容器可以公平地分享主机的内存、CPU、磁盘 IO 等资源；当然，更重要的是，控制组确保了当容器内的资源使用产生压力时不会连累主机系统 尽管控制组不负责隔离容器之间相互访问、处理数据和进程，它在防止拒绝服务（DDOS）攻击方面是必不可少的。尤其是在多用户的平台（比如公有或私有的 PaaS）上，控制组十分重要。例如，当某些应用程序表现异常的时候，可以保证一致地正常运行和性能 服务端防护运行一个容器或应用程序的核心是通过 Docker 服务端。Docker 服务的运行目前需要 root 权限，因此其安全性十分关键 首先，确保只有可信的用户才可以访问 Docker 服务。Docker 允许用户在主机和容器间共享文件夹，同时不需要限制容器的访问权限，这就容易让容器突破资源限制 最近改进的 Linux 命名空间机制将可以实现使用非 root 用户来运行全功能的容器。这将从根本上解决了容器和主机之间共享文件系统而引起的安全问题 终极目标是改进 2 个重要的安全特性： 将容器的 root 用户映射到本地主机上的非 root 用户，减轻容器和主机之间因权限提升而引起的安全问题； 允许 Docker 服务端在非 root 权限下运行，利用安全可靠的子进程来代理执行需要特权权限的操作。这些子进程将只允许在限定范围内进行操作，例如仅仅负责虚拟网络设定或文件系统管理、配置操作等 内核能力机制能力机制（Capability）是 Linux 内核一个强大的特性，可以提供细粒度的权限访问控制。 Linux 内核自 2.2 版本起就支持能力机制，它将权限划分为更加细粒度的操作能力，既可以作用在进程上，也可以作用在文件上 默认情况下，Docker 启动的容器被严格限制只允许使用内核的一部分能力 大部分情况下，容器并不需要“真正的” root 权限，容器只需要少数的能力即可。为了加强安全，容器可以禁用一些没必要的权限 默认情况下，Docker采用白名单机制，禁用必需功能之外的其它权限。 当然，用户也可以根据自身需求来为 Docker 容器启用额外的权限 其他安全特性除了能力机制之外，还可以利用一些现有的安全机制来增强使用 Docker 的安全性，例如 TOMOYO, AppArmor, SELinux, GRSEC 等。 Docker 当前默认只启用了能力机制。用户可以采用多种方案来加强 Docker 主机的安全，例如： 在内核中启用 GRSEC 和 PAX，这将增加很多编译和运行时的安全检查；通过地址随机化避免恶意探测等。并且，启用该特性不需要 Docker 进行任何配置。 使用一些有增强安全特性的容器模板，比如带 AppArmor 的模板和 Redhat 带 SELinux 策略的模板。这些模板提供了额外的安全特性。 用户可以自定义访问控制机制来定制安全策略。 跟其它添加到 Docker 容器的第三方工具一样（比如网络拓扑和文件系统共享），有很多类似的机制，在不改变 Docker 内核情况下就可以加固现有的容器 总结总体来看，Docker 容器还是十分安全的，特别是在容器内不使用 root 权限来运行进程的话。 另外，用户可以使用现有工具，比如 Apparmor, SELinux, GRSEC 来增强安全性；甚至自己在内核中实现更复杂的安全机制 Docker 底层实现Docker 底层的核心技术包括 Linux 上的命名空间（Namespaces）、控制组（Control groups）、Union 文件系统（Union file systems）和容器格式（Container format） 传统的虚拟机通过在宿主主机中运行 hypervisor 来模拟一整套完整的硬件环境提供给虚拟机的操作系统。虚拟机系统看到的环境是可限制的，也是彼此隔离的。 这种直接的做法实现了对资源最完整的封装，但很多时候往往意味着系统资源的浪费。 例如，以宿主机和虚拟机系统都为 Linux 系统为例，虚拟机中运行的应用其实可以利用宿主机系统中的运行环境 在操作系统中，包括内核、文件系统、网络、PID、UID、IPC、内存、硬盘、CPU 等等，所有的资源都是应用进程直接共享的。 要想实现虚拟化，除了要实现对内存、CPU、网络IO、硬盘IO、存储空间等的限制外，还要实现文件系统、网络、PID、UID、IPC等等的相互隔离。 前者相对容易实现一些，后者则需要宿主机系统的深入支持 随着 Linux 系统对于命名空间功能的完善实现，程序员已经可以实现上面的所有需求，让某些进程在彼此隔离的命名空间中运行。大家虽然都共用一个内核和某些运行时环境（例如一些系统命令和系统库），但是彼此却看不到，都以为系统中只有自己的存在。这种机制就是容器（Container），利用命名空间来做权限的隔离控制，利用 cgroups 来做资源分配 Docker 基本架构Docker 采用了 C/S 架构，包括客户端和服务端。Docker 守护进程 （Daemon）作为服务端接受来自客户端的请求，并处理这些请求（创建、运行、分发容器） 客户端和服务端既可以运行在一个机器上，也可通过 socket 或者 RESTful API 来进行通信 Docker 守护进程一般在宿主主机后台运行，等待接收来自客户端的消息。 Docker 客户端则为用户提供一系列可执行命令，用户用这些命令实现跟 Docker 守护进程交互 命名空间命名空间是 Linux 内核一个强大的特性。每个容器都有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼此互不影响 PID 命名空间 不同用户的进程就是通过 pid 命名空间隔离开的，且不同命名空间中可以有相同 pid。所有的 LXC 进程在 Docker 中的父进程为Docker进程，每个 LXC 进程具有不同的命名空间。同时由于允许嵌套，因此可以很方便的实现嵌套的 Docker 容器 NET 命名空间 有了 pid 命名空间, 每个命名空间中的 pid 能够相互隔离，但是网络端口还是共享 host 的端口。网络隔离是通过 net 命名空间实现的， 每个 net 命名空间有独立的 网络设备, IP 地址, 路由表, /proc/net 目录。这样每个容器的网络就能隔离开来。Docker 默认采用 veth 的方式，将容器中的虚拟网卡同 host 上的一 个Docker 网桥 docker0 连接在一起 IPC 命名空间 容器中进程交互还是采用了 Linux 常见的进程间交互方法(interprocess communication - IPC), 包括信号量、消息队列和共享内存等。然而同 VM 不同的是，容器的进程间交互实际上还是 host 上具有相同 pid 命名空间中的进程间交互，因此需要在 IPC 资源申请时加入命名空间信息，每个 IPC 资源有一个唯一的 32 位 id mnt 命名空间 类似 chroot，将一个进程放到一个特定的目录执行。mnt 命名空间允许不同命名空间的进程看到的文件结构不同，这样每个命名空间 中的进程所看到的文件目录就被隔离开了。同 chroot 不同，每个命名空间中的容器在 /proc/mounts 的信息只包含所在命名空间的 mount point uts 命名空间 UTS(“UNIX Time-sharing System”) 命名空间允许每个容器拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 主机上的一个进程 user 命名空间 每个容器可以有不同的用户和组 id, 也就是说可以在容器内用容器内部的用户执行程序而非主机上的用户 控制组控制组（cgroups）是 Linux 内核的一个特性，主要用来对共享资源进行隔离、限制、审计等。只有能控制分配到容器的资源，才能避免当多个容器同时运行时的对系统资源的竞争。控制组技术最早是由 Google 的程序员在 2006 年提出，Linux 内核自 2.6.24 开始支持。控制组可以提供对容器的内存、CPU、磁盘 IO 等资源的限制和审计管理 联合文件系统联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem) 联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像 另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率 Docker 目前支持的联合文件系统包括 OverlayFS, AUFS, Btrfs, VFS, ZFS 和 Device Mapper 容器格式最初，Docker 采用了 LXC 中的容器格式。从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd 网络实现Docker 的网络实现其实就是利用了 Linux 上的网络命名空间和虚拟网络设备（特别是 veth pair） 首先，要实现网络通信，机器需要至少一个网络接口（物理接口或虚拟接口）来收发数据包；此外，如果不同子网之间要进行通信，需要路由机制 Docker 中的网络接口默认都是虚拟的接口。虚拟接口的优势之一是转发效率较高。 Linux 通过在内核中进行数据复制来实现虚拟接口之间的数据转发，发送接口的发送缓存中的数据包被直接复制到接收接口的接收缓存中。对于本地系统和容器内系统看来就像是一个正常的以太网卡，只是它不需要真正同外部网络设备通信，速度要快很多 Docker 容器网络就利用了这项技术。它在本地主机和容器内分别创建一个虚拟接口，并让它们彼此连通（这样的一对接口叫做 veth pair） Docker 项目etcd：分布式键值数据库etcd是CoreOS团队于 2013 年 6 月发起的开源项目，它的目标是构建一个高可用的分布式键值（key-value）数据库，基于Go语言实现。我们知道，在分布式系统中，各种服务的配置信息的管理分享，服务的发现是一个很基本同时也是很重要的问题。CoreOS项目就希望基于etcd` 来解决这一问题 受到 Apache ZooKeeper 项目和 doozer 项目的启发，etcd 在设计的时候重点考虑了下面四个要素： 简单：具有定义良好、面向用户的 API (gRPC) 安全：支持 HTTPS 方式的访问 快速：支持并发 10 k/s 的写操作 可靠：支持分布式结构，基于 Raft 的一致性算法 Apache ZooKeeper 是一套知名的分布式系统中进行同步和一致性管理的工具。 doozer 是一个一致性分布式数据库。 Raft 是一套通过选举主节点来实现分布式系统一致性的算法，相比于大名鼎鼎的 Paxos 算法，它的过程更容易被人理解，由 Stanford 大学的 Diego Ongaro 和 John Ousterhout 提出。更多细节可以参考 raftconsensus.github.io。 一般情况下，用户使用 etcd 可以在多个节点上启动多个实例，并添加它们为一个集群。同一个集群中的 etcd 实例将会保持彼此信息的一致性 CoreOS：基于容器的 OSCoreOS 的设计是为你提供能够像谷歌一样的大型互联网公司一样的基础设施管理能力来动态扩展和管理的计算能力 CoreOS 的安装文件和运行依赖非常小,它提供了精简的 Linux 系统。它使用 Linux 容器在更高的抽象层来管理你的服务，而不是通过常规的 YUM 和 APT 来安装包。 同时，CoreOS 几乎可以运行在任何平台：VirtualBox, Amazon EC2, QEMU/KVM, VMware 和 OpenStack 等等，甚至你所使用的硬件环境 其提供了运行现代基础设施的特性，支持大规模服务部署，使得在基于最小化的现代操作系统上构建规模化的计算仓库成为了可能 最小化OS：CoreOS 被设计成一个基于容器的最小化的现代操作系统。它比现有的 Linux 安装平均节省 40% 的 RAM（大约 114M ）并允许从 PXE 或 iPXE 非常快速的启动 无痛更新：利用主动和被动双分区方案来更新 OS，使用分区作为一个单元而不是一个包一个包的更新。这使得每次更新变得快速，可靠，而且很容易回滚 Docker 容器：应用作为 Docker 容器运行在 CoreOS 上。容器以包的形式提供最大得灵活性并且可以在几毫秒启动 支持集群：CoreOS 可以在一个机器上很好地运行，但是它被设计用来搭建集群。可以通过 k8s 很容易得使应用容器部署在多台机器上并且通过服务发现把他们连接在一起 分布式系统工具：内置诸如分布式锁和主选举等原生工具用来构建大规模分布式系统得构建模块 服务发现：容易定位服务在集群的那里运行并当发生变化时进行通知。它是复杂高动态集群必不可少的。在 CoreOS 中构建高可用和自动故障负载 Kubernetes：容器集群管理系统是 Google 团队发起并维护的基于 Docker 的开源容器集群管理系统 ，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为 Go 语言 易学：轻量级，简单，容易理解 便携：支持公有云，私有云，混合云，以及多种云平台 可拓展：模块化，可插拔，支持钩子，可任意组合 自修复：自动重调度，自动重启，自动复制 在分布式系统中，部署，调度，伸缩一直是最为重要的也最为基础的功能，Kubernetes 就是希望解决这一序列问题的 建于 Docker 之上的 Kubernetes 可以构建一个容器的调度服务，其目的是让用户透过 Kubernetes 集群来进行云端容器集群的管理，而无需用户进行复杂的设置工作。系统会自动选取合适的工作节点来执行具体的容器集群调度处理工作 其核心概念是 Container Pod。一个 Pod 由一组工作于同一物理工作节点的容器构成。这些组容器拥有相同的网络命名空间、IP以及存储配额，也可以根据实际情况对每一个 Pod 进行端口映射。此外，Kubernetes 工作节点会由主系统进行管理，节点包含了能够运行 Docker 容器所用到的服务 Mesos：集群资源调度平台Mesos 项目是源自 UC Berkeley 的对集群资源进行抽象和管理的开源项目，类似于操作系统内核，用户可以使用它很容易地实现分布式应用的自动化调度。可以将整个数据中心的资源（包括 CPU、内存、存储、网络等）进行抽象和调度，使得多个应用同时运行在集群中分享资源，并无需关心资源的物理分布情况 如果把数据中心中的集群资源看做一台服务器，那么 Mesos 要做的事情，其实就是今天操作系统内核的职责：抽象资源 + 调度任务。Mesos 项目是 Mesosphere 公司 Datacenter Operating System (DCOS) 产品的核心部件 Mesos 拥有许多引人注目的特性，包括： 支持数万个节点的大规模场景（Apple、Twitter、eBay 等公司实践）； 支持多种应用框架，包括 Marathon、Singularity、Aurora 等； 支持 HA（基于 ZooKeeper 实现）； 支持 Docker、LXC 等容器机制进行任务隔离； 提供了多个流行语言的 API，包括 Python、Java、C++ 等； 自带了简洁易用的 WebUI，方便用户直接进行操作。 值得注意的是，Mesos 自身只是一个资源抽象的平台，要使用它往往需要结合运行其上的分布式应用（在 Mesos 中被称作框架，framework），比如 Hadoop、Spark 等可以进行分布式计算的大数据处理应用；比如 Marathon 可以实现 PaaS，快速部署应用并自动保持运行；比如 ElasticSearch 可以索引海量数据，提供灵活的整合和查询能力…… 常见应用框架： 长期运行的服务 Aurora Marathon Singularity 大数据处理 Cray Chapel Dpark Hadoop Spark Storm 批量调度 Chronos Jenkins JobServer GoDocker 数据存储 ElasticSearch Hypertable Tachyon Mesos 最初设计为资源调度器，然而其灵活的设计和对上层框架的优秀支持，使得它可以很好的支持大规模的分布式应用场景。结合 Docker，Mesos 可以很容易部署一套私有的容器云。 除了核心功能之外，Mesos 在设计上有许多值得借鉴之处，比如它清晰的定位、简洁的架构、细致的参数、高度容错的可靠，还有对限速、监控等的支持等。 Mesos 作为一套成熟的开源项目，可以很好的被应用和集成到生产环境中。但它的定位集中在资源调度，往往需要结合应用框架或二次开发 Docker 与云计算目前与容器相关的云计算主要分为两种类型。 一种是传统的 IaaS 服务商提供对容器相关的服务，包括镜像下载、容器托管等。 另一种是直接基于容器技术对外提供容器云服务，所谓 Container as a Service（CaaS） 使用 Docker 的云服务厂商有： AWS：EC2 腾讯云 阿里云 Q&amp;ADocker 与 LXC（Linux Container）有何不同？答：LXC 利用 Linux 上相关技术实现了容器。Docker 则在如下的几个方面进行了改进： 移植性：通过抽象容器配置，容器可以实现从一个平台移植到另一个平台； 镜像系统：基于 AUFS 的镜像系统为容器的分发带来了很多的便利，同时共同的镜像层只需要存储一份，实现高效率的存储； 版本管理：类似于Git的版本管理理念，用户可以更方便的创建、管理镜像文件； 仓库系统：仓库系统大大降低了镜像的分发和管理的成本； 周边工具：各种现有工具（配置管理、云平台）对 Docker 的支持，以及基于 Docker的 PaaS、CI 等系统，让 Docker 的应用更加方便和多样化 如何将一台宿主主机的 Docker 环境迁移到另外一台宿主主机？答：停止 Docker 服务。将整个 Docker 存储文件夹复制到另外一台宿主主机，然后调整另外一台宿主主机的配置即可 Docker 命令查询基本语法Docker 命令有两大类，客户端命令和服务端命令。前者是主要的操作接口，后者用来启动 Docker Daemon。 客户端命令：基本命令格式为 docker [OPTIONS] COMMAND [arg...]； 服务端命令：基本命令格式为 dockerd [OPTIONS]。 可以通过 man docker 或 docker help 来查看这些命令。 客户端命令选项 --config=&quot;&quot;：指定客户端配置文件，默认为 ~/.docker； -D=true|false：是否使用 debug 模式。默认不开启； -H, --host=[]：指定命令对应 Docker 守护进程的监听接口，可以为 unix 套接字 unix:///path/to/socket，文件句柄 fd://socketfd 或 tcp 套接字 tcp://[host[:port]]，默认为 unix:///var/run/docker.sock； -l, --log-level=&quot;debug|info|warn|error|fatal&quot;：指定日志输出级别； --tls=true|false：是否对 Docker 守护进程启用 TLS 安全机制，默认为否； --tlscacert=/.docker/ca.pem：TLS CA 签名的可信证书文件路径； --tlscert=/.docker/cert.pem：TLS 可信证书文件路径； --tlscert=/.docker/key.pem：TLS 密钥文件路径； --tlsverify=true|false：启用 TLS 校验，默认为否。 dockerd 命令选项 --api-cors-header=&quot;&quot;：CORS 头部域，默认不允许 CORS，要允许任意的跨域访问，可以指定为 “*”； --authorization-plugin=&quot;&quot;：载入认证的插件； -b=&quot;&quot;：将容器挂载到一个已存在的网桥上。指定为 none 时则禁用容器的网络，与 --bip 选项互斥； --bip=&quot;&quot;：让动态创建的 docker0 网桥采用给定的 CIDR 地址; 与 -b 选项互斥； --cgroup-parent=&quot;&quot;：指定 cgroup 的父组，默认 fs cgroup 驱动为 /docker，systemd cgroup 驱动为 system.slice； --cluster-store=&quot;&quot;：构成集群（如 Swarm）时，集群键值数据库服务地址； --cluster-advertise=&quot;&quot;：构成集群时，自身的被访问地址，可以为 host:port 或 interface:port； --cluster-store-opt=&quot;&quot;：构成集群时，键值数据库的配置选项； --config-file=&quot;/etc/docker/daemon.json&quot;：daemon 配置文件路径； --containerd=&quot;&quot;：containerd 文件的路径； -D, --debug=true|false：是否使用 Debug 模式。缺省为 false； --default-gateway=&quot;&quot;：容器的 IPv4 网关地址，必须在网桥的子网段内； --default-gateway-v6=&quot;&quot;：容器的 IPv6 网关地址； --default-ulimit=[]：默认的 ulimit 值； --disable-legacy-registry=true|false：是否允许访问旧版本的镜像仓库服务器； --dns=&quot;&quot;：指定容器使用的 DNS 服务器地址； --dns-opt=&quot;&quot;：DNS 选项； --dns-search=[]：DNS 搜索域； --exec-opt=[]：运行时的执行选项； --exec-root=&quot;&quot;：容器执行状态文件的根路径，默认为 /var/run/docker； --fixed-cidr=&quot;&quot;：限定分配 IPv4 地址范围； --fixed-cidr-v6=&quot;&quot;：限定分配 IPv6 地址范围； -G, --group=&quot;&quot;：分配给 unix 套接字的组，默认为 docker； -g, --graph=&quot;&quot;：Docker 运行时的根路径，默认为 /var/lib/docker； -H, --host=[]：指定命令对应 Docker daemon 的监听接口，可以为 unix 套接字 unix:///path/to/socket，文件句柄 fd://socketfd 或 tcp 套接字 tcp://[host[:port]]，默认为 unix:///var/run/docker.sock； --icc=true|false：是否启用容器间以及跟 daemon 所在主机的通信。默认为 true。 --insecure-registry=[]：允许访问给定的非安全仓库服务； --ip=&quot;&quot;：绑定容器端口时候的默认 IP 地址。缺省为 0.0.0.0； --ip-forward=true|false：是否检查启动在 Docker 主机上的启用 IP 转发服务，默认开启。注意关闭该选项将不对系统转发能力进行任何检查修改； --ip-masq=true|false：是否进行地址伪装，用于容器访问外部网络，默认开启； --iptables=true|false：是否允许 Docker 添加 iptables 规则。缺省为 true； --ipv6=true|false：是否启用 IPv6 支持，默认关闭； -l, --log-level=&quot;debug|info|warn|error|fatal&quot;：指定日志输出级别； --label=&quot;[]&quot;：添加指定的键值对标注； --log-driver=&quot;json-file|syslog|journald|gelf|fluentd|awslogs|splunk|etwlogs|gcplogs|none&quot;：指定日志后端驱动，默认为 json-file； --log-opt=[]：日志后端的选项； --mtu=VALUE：指定容器网络的 mtu； -p=&quot;&quot;：指定 daemon 的 PID 文件路径。缺省为 /var/run/docker.pid； --raw-logs：输出原始，未加色彩的日志信息； --registry-mirror=&lt;scheme&gt;://&lt;host&gt;：指定 docker pull 时使用的注册服务器镜像地址； -s, --storage-driver=&quot;&quot;：指定使用给定的存储后端； --selinux-enabled=true|false：是否启用 SELinux 支持。缺省值为 false。SELinux 目前尚不支持 overlay 存储驱动； --storage-opt=[]：驱动后端选项； --tls=true|false：是否对 Docker daemon 启用 TLS 安全机制，默认为否； --tlscacert=/.docker/ca.pem：TLS CA 签名的可信证书文件路径； --tlscert=/.docker/cert.pem：TLS 可信证书文件路径； --tlscert=/.docker/key.pem：TLS 密钥文件路径； --tlsverify=true|false：启用 TLS 校验，默认为否； --userland-proxy=true|false：是否使用用户态代理来实现容器间和出容器的回环通信，默认为 true； --userns-remap=default|uid:gid|user:group|user|uid：指定容器的用户命名空间，默认是创建新的 UID 和 GID 映射到容器内进程。 客户端命令可以通过 docker COMMAND --help 来查看这些命令的具体用法。 attach：依附到一个正在运行的容器中； build：从一个 Dockerfile 创建一个镜像； commit：从一个容器的修改中创建一个新的镜像； cp：在容器和本地宿主系统之间复制文件中； create：创建一个新容器，但并不运行它； diff：检查一个容器内文件系统的修改，包括修改和增加； events：从服务端获取实时的事件； exec：在运行的容器内执行命令； export：导出容器内容为一个 tar 包； history：显示一个镜像的历史信息； images：列出存在的镜像； import：导入一个文件（典型为 tar 包）路径或目录来创建一个本地镜像； info：显示一些相关的系统信息； inspect：显示一个容器的具体配置信息； kill：关闭一个运行中的容器 (包括进程和所有相关资源)； load：从一个 tar 包中加载一个镜像； login：注册或登录到一个 Docker 的仓库服务器； logout：从 Docker 的仓库服务器登出； logs：获取容器的 log 信息； network：管理 Docker 的网络，包括查看、创建、删除、挂载、卸载等； node：管理 swarm 集群中的节点，包括查看、更新、删除、提升/取消管理节点等； pause：暂停一个容器中的所有进程； port：查找一个 nat 到一个私有网口的公共口； ps：列出主机上的容器； pull：从一个Docker的仓库服务器下拉一个镜像或仓库； push：将一个镜像或者仓库推送到一个 Docker 的注册服务器； rename：重命名一个容器； restart：重启一个运行中的容器； rm：删除给定的若干个容器； rmi：删除给定的若干个镜像； run：创建一个新容器，并在其中运行给定命令； save：保存一个镜像为 tar 包文件； search：在 Docker index 中搜索一个镜像； service：管理 Docker 所启动的应用服务，包括创建、更新、删除等； start：启动一个容器； stats：输出（一个或多个）容器的资源使用统计信息； stop：终止一个运行中的容器； swarm：管理 Docker swarm 集群，包括创建、加入、退出、更新等； tag：为一个镜像打标签； top：查看一个容器中的正在运行的进程信息； unpause：将一个容器内所有的进程从暂停状态中恢复； update：更新指定的若干容器的配置信息； version：输出 Docker 的版本信息； volume：管理 Docker volume，包括查看、创建、删除等； wait：阻塞直到一个容器终止，然后输出它的退出符 Reference https://yeasy.gitbooks.io/docker_practice/ Docker 从入门到实战-黄靖钧-华章出版社]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>分布式系统</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何生成网络流]]></title>
    <url>%2F2019%2F03%2F05%2Fflow-generation%2F</url>
    <content type="text"><![CDATA[传统网络中的二层帧、三层报文、四层 TCP流，UDP 数据报在 SDN 中都统称为流（flow）。流是 SDN 网络中最基本的东西，在搭建 SDN 实验环境，编写 SDN 控制器，测试网络连通性和网络性能等任务时你都可以不使用到“流”，只有到仿真的时候你才会切实地需要流，真实环境的流。有趣的是，在创造“流”的过程中我也花了很多时间——遇到了一个逻辑上的麻烦。 什么是流啊？怎么生成流啊？怎么生成真实环境的流啊？ 这三个问题困扰了我好几天，我所参考的 SDN 论文、书籍、文章、文档中都默认流就在系统中了，但是对于系统是如何生成和使用流的，似乎所有人都是“看，那里有一条流，我们将其输入系统即可”。文章上是这么写的，但是对于我的实验来说就是“看，那里什么都没有”。 那么，流从哪里来呢？资料说，流从 iperf 来，流从 ping 中来。好家伙，iperf 不是使用 TCP 流测试网络带宽的吗？ping 不是使用 ICMP 测试网络连通性的吗？怎么还能用来生成流了？ 带着这样的疑问我又思考了一段时间，恍然大悟。 原来答案一直就在流的定义里：传统网路中的二层帧、三层报文、四层 TCP流，UDP 数据报在 SDN 中都统称为流。根据仿真业务的需要生成特定的数据在 SDN 网络中就是我们所需要的流，因此，凡是能生成网络数据的工具都能用来生成流，比如: 1ping, iperf/iperf3, wget, curl, netperf, netcat, linux web packets, python scapy… OK，知道了怎么生成流，那生成的流需要具有些什么特征呢？ 不同的论文根据其研究范围对流的特征有不同的要求，一般选择四层数据构成的五元组来定义一条流： 1[src_ip, dst_ip, proto_type, src_port, dst_port] 从 openflow 出发，我们可以匹配更多的特征： 二层：链路层承载的协议，帧长度 三层：网络层承载的协议，ip拓展字段，报文长度 四层：tcp 拓展字段，udp 拓展字段 包括但不限于上面列举的特征，更多内容在 Openflow 标准中可以找到。 这里可以逆向思考：OpenFlow 能匹配啥，我就能生成啥。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>SDN</tag>
        <tag>flow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fat Tree 拓扑]]></title>
    <url>%2F2019%2F03%2F05%2Ffat-tree-topo%2F</url>
    <content type="text"><![CDATA[随着 ICT 技术的发展，数据中心网络（DCN）已经成为当今信息社会的基础设施，作为一种网络形态，其网络需求和网络拓扑表现出一定的特点： 高度可拓展性：随着业务的扩大平稳地接入更多的服务器和交换机，实现性能的平稳扩展 多路径：避免单点故障，实现链路的负载均衡 低时延：考虑到低时延业务的需求 高带宽：满足业务的高吞吐率需求 网络互联开销低：使用便宜的低端交换机代替昂贵的专用交换机 模块化设计：利用模块化设计来降低网络运维的复杂度 网络扁平化：构建网络的层数尽可能低，以利于网络的流量均衡，方便管理 绿色节能：合理的网络布局有利于网络降低能耗，保护网络设备 Fat Tree 拓扑是一种常用于数据中心网络（DCN）的拓扑结构，它是对传统 Tree 拓扑的改进，下文按照技术演进过程推演 Fat Tree 拓扑。 从 Tree 到 Fat Tree在 DCN 中，根据网络中负责转发数据的设备不同，可以将网络拓扑分为：Switch-only、Server-only 和混合型拓扑。 Tree 拓扑该拓扑用于早期 DCN 中，该拓扑的结构特点为： 多根树结构 switch-only 型 底层采用商用交换机与服服务器连接，高层采用专用交换设备 该拓扑的优点为： 结构简单，易于实现 该拓扑的缺点为： 采用垂直扩展方式实现扩展，由于高层交换机的端口数目限制，拓展的深度有限，难以满足现代 DCN 需求（2 层结构能容纳 5k～8k 个服务器） 网络容错性能差，网络中的节点或者链路故障容易切分网络 流量分布不均匀，容易在核心节点处汇聚，使核心节点成为性能瓶颈 专用交换机采购开销巨大 Fat Tree 拓扑 Fat Tree 拓扑是由 MIT 的 Al-Fares 等人在改进传统 Tree 结构的基础上提出的。该拓扑的结构特点为： 网络分为三层，自下而上分别为：Edge, Aggregate, Core 汇聚层交换机与边缘交换机构成一个 Pod 所有交换机均采用商用交换机 拓扑和核心参数为：Pod 数 k 和每个 ToR 交换机下 Server 的接入数 density，拓扑的构建规则如下： 国内的研究对下面的内翻译大同小异，这里按原汁原味的来 k pods for each fat tree topo. k port for each switch, witch means density = k/2 k/2 edge switches and k/2 aggregate switches for each pod (k/2)^2 core switch k/2 servers or hosts in each edge switch, witch means (k/2)^2 hosts in each pod, (k^3)/4 hosts for the topo. servers connect to its own ToR edge switch edge switch connect to all aggregate switch in the same pod aggregate connect to core switch by special method 拓扑的性能特征为： 网络直径 2logk 并行链路 (k/2)^2 扩展能力 (k^3)/4 拓扑的优点为： 网络采用水平扩展方式，通过横向增加网络中的 pod 就能使网络支持更多的服务器 对分带宽随着网络规模的扩大而扩大 在核心层使用多条链路实现负载均衡，不同 pod 之间有多条并行路径（ECMP） 在 pod 内合理分流优化网络负载 网络容错性能好，一般不会出现单点故障 网络直径小，这意味着网络理论时延小，实时性好 拓扑规则，利于模块化设计和拓展 采用商用交换机，大幅降低网络设备 的开销 拓扑的缺点： 理论上其拓展性能受限于交换机端口数 k Pod 内拓扑的容错性能差 网络不能很好的支持 one-to-all，all-to-all 通信模式，不利于部署 MapReduce 等现代高性能应用 网络中交换机与服务器比值较大 总结现在再回过头来看看我们的胖树拓扑，为什么是胖？ Tree 拓扑中带宽是逐层收敛的，树根处的带宽远小于叶子处所有带宽之和，而 Fat Tree 中越到树根，枝干越粗 拓扑的扩展是横向进行了，从树的角度来思考，树变宽了而不是变深了 任意节点的上下行带宽是相等的 不同 pod 之间的距离是相等的，这意味着网络的时延很小，且取值很少 试验模拟 4 个 pod 的 Fat-Tree 拓扑，拓扑参数如下： 20 个交换机，每个交换机有 4 个接口 4 个核心交换机（core switch， cs），编号 cs001~cs004 8 个汇聚交换机（aggregate switch， as），编号 as001~as008 8 个边缘交换机（edge switch，es），也称为 ToR 交换机，编号 es001~es008 每个 es 接入 2 个 host（density=2），一共 16 个 host 我们对 host 进行了资源限制，每个 host 使用 1/16 的 CPU 处理能力 48 条链路 链路时延 1ms 带宽 host-es 100M，es-as 500M, as-cs 1000M （经过验证，带宽对我们的实验没有影响） 我们对 host 设计了新的 IPv4 地址 [10.pod.tor.index]，其中： 10 为公共前缀 pod 代表 host 所属的 pod，k=4 时 pod 可取 [1, 2, 3, 4] tor 代表 host 所属的 tor 交换机，k=4 时 tor 可取 [1, 2] index 代表该 tor 下的 host 序号，k=4 时 index 可取 [1, 2] 举例: IPv4 = 10.2.1.2 代表 pod 2 的 es1 下面的第 2 个 host 我们对 switch 设计了新的 datapath id 方便控制器区分和访问： 同一个 pod 中的 as 和 es 连续编号 不同 pod 之间采用 第一位进行区分 123core_dpid_list = [1, 2, 3, 4]aggr_dpid_list = [12, 13, 22, 23, 32, 33, 42, 43]edge_dpid_list = [10, 11, 20, 21, 30, 31, 40, 41] 交换机采用 openflow 1.3.0 协议 k4 Fat Tree 拓扑有如下特点： 任意两点之间都存在多条可达且等价的路径，因此 Fat Tree Topo 上可以使用 ECMP 和 FRR 策略，本实验不使用这两种方法 后记由于目前已有资料中对 Fat Tree 拓扑实现的源码资料很少，实现方法也各不相同，在这里我给出我的实现 github。 这里针对拓扑的实现说一点题外话，在 SDN 试验中，构建拓扑时我们需要想清楚是用二层交换还是三层路由的思路来保证互联互通，虽然区分层次和 SDN 的思想相违背，但是，网络设备连接的还是传统的层次网络终端，因此他们的网络行为还是分层的。具体来说，就涉及到了 IP 地址的分配和子网的设计（在我的实现中，每个 ToR 为一个子网，每个 pod 为一个子网）。 同样的道理，如果希望制定精确的流表规则，我们也要针对不同的层次、协议进行匹配。 参考资料 ToR: Top of Rank, 机柜接入交换机，相当于介入交换机或者 leaf switch https://en.wikipedia.org/wiki/Fat_tree https://www.cnblogs.com/zhuting/p/8880475.html https://www.cnblogs.com/hdk1993/p/4571824.html https://wenku.baidu.com/view/b8b6cd595727a5e9846a6164.html]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>SDN</tag>
        <tag>DCN</tag>
        <tag>Topo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Segment Routing 协议浅析]]></title>
    <url>%2F2019%2F03%2F05%2Fsegment-routing%2F</url>
    <content type="text"><![CDATA[在 Cisco Segment Routing 的中文官网上，思科用 8 个字总结了这项意义深远的 TE 技术 化繁为简 无远弗届 。 化繁为简，指越复杂的事情越是可以用简单的方法去化解。 无远弗届，指不管多远，没有到不了的。 在这里，思科用化繁为简表达 SR 技术对过去流量工程所积累下来的复杂状态的突破，用无远弗届来暗指路由技术的本质。 本文会结合我的实践对 SR 技术进行简单的介绍，并对技术背后的设计思想做有限的揣度。关于 SR 技术的细节推荐大家直接去看 Clarence Filsfils 的《Segment Routing 详解》，他是 SR 技术的发明者。 出发：化繁为简在开始我们的旅程之前，我们先回顾一些简单的网络知识，通过几个问题来介绍旅程的背景知识。 终端产生的数据是如何通过网络到达目的终端的？ 我们先来看一个形象的例子： 终端产生的数据被打包成报文的形式交给网络，这个过程类似于你将文件交给快递公司，由快递公司将文件装进特定的包装里并打上相应的标记 网络就像城市一样，从一点到另一点可能存在很多种走法。网络中的每个岔路口都有一个网络设备在站岗，每当快递员带着包裹来问路的时候就按自己手上的地图告诉快递员从哪条路出去最近 带着包裹的快递员每到一个岔路就问一次，他总是选择当前位置到目标位置的最短路线向着目标进发 上面介绍的就是由网络提供的路由技术，它完成了数据包的传递工作。下面我们结合具体的路由技术来看看上面的过程。 如果这个快递公司只为一个很小的城市提供服务，并且这个城市已经发展很成熟了，不再需要大兴土木搞城市建设了。如果他能记得这个城市所有的路口并为所有的用户提供准备好的运输路线，那么这个快递公司就不需要再采用向路口的网络设备问路了，快递公司只需要在每个路口建一个路标，快递员自己查看路标就行了——这就是静态路由算法，由网络管理员提前在网络设备上配置好各自的路由表。 如果这个快递公司为一个大型城市服务，或者这个城市的道路情况经常发生变化（随机修路），如果快递公司继续采用静态路由算法，由于城市链接情况经常改变，为了保证公司的业务水平，快递公司必须经常修改他的运输地图——这就是动态路由算法。在动态路由算法中发展出了两种主要的策略： 一种是每次网络更新时所有人告诉自己的邻居自己的连接情况，邻居再把他收集到的情况汇总后告诉他的邻居，以此类推，最终，网络中的所有设备都会得到整个网络的连接情况，每个设备单独地用 dijkstra 算法算出网络中的最短路情况，这就是 OSPF 协议 另一种是网络设备收到邻居的信息后主动计算当前局部地图上的最短路情况，并把这个情况告诉邻居的邻居，这就是 RIP 协议 基于上面两种路由算法我们可以得到具体的路由协议，比如 IGP, BGP, IS-IS 等等其内容会变得更加复杂，但他们都遵循着一条最简单的信条——最短路。 这就是传统网络中使用的路由协议，网络总是将包裹发送到距离终点最短的下一个十字路口。 最短路会产生什么问题吗？ 最短路作为一个优化目标是最符合直觉的，它是人类生活中最常用的一种优化策略。但是最短路也会引发一些问题，比如： 你并不知道你的报文经过了哪些节点 人人都选择最短路，那么在必经之路上就会产生严重的拥塞 结合上面两点，发生了拥塞之后你也很难去纾解拥塞 OK，最后我们来总结一下传统网络中数据的传输策略——网络设备采用分布式决策为包裹选择自己认为的最短路进行存储转发。 还有没有其他方法？ 为了招揽更多的客户，快递公司提供了一项 VIP 服务：用户可以提供一张站点表，上面一次写上用户希望包裹经过的路口，快递公司将这个表贴在包裹外面，要求快递员必须按照表上的路线送达——这就是源路由协议，用户可以指定其所发送的数据包沿途经过的部分或者全部路由器。 本文介绍的 Segment Routing 就是一种源路由机制，如果说 MPLS 是指定经过的所有节点的话，SR 就是指定路途上的关键节点，关键节点之间依然采用最短路策略。 对于 MPLS 协议，我们提前为用户选择的路设定一个标签，当匹配的包裹进入网络是我们就给他打上这个标签，包裹拿着这个标签穿越 MPLS 网络，在出口处摘下标签，由边缘路由器将包裹交给目标网络继续转发。 对于 SR 协议，我们将网络中所有的设备和线路标号，当用户包裹进入网络时根据用户的需求选择一些关键路线和关键节点贴到包裹上构成标签栈，包裹按照标签栈的指示依次按最短路到达栈顶标签所指示的位置，由相应网络设备弹出其栈顶标签，包裹继续转发到下一栈顶标签的位置，以此重复，直到离开 SR 网络。 通过上面的介绍，我们可以发现源路由协议与传统路由协议有一个很大的区别：可控。 也正是这一点，源路由协议非常适合在 SDN 网络中部署，过去在分布式网络中很复杂的决策过程在 SDN 网络中变得非常简单。 我们怎么使用 Segement Routing？ 经过我的搜索，目前 SR 技术有两种实现载体 MPLS 和 IPv6 SR on IPv6 (SRv6) 这个方案的关键细节如下： 使用 v6 拓展头部（Routing Header） SRH 中包含一个 Segment List，编码为 IPv6 address，每个地址代表一个节点或者一条邻接关系 SRH 中含有一个 Segments Left 指针，指示当前活动的 Segment SRH 中可以使用 TLV 总的来说，就是利用了 IPv6 协议的扩展头部来实现 Segment List，思路很直观，但是使用起来就很令人抓狂，这是因为在我的仿真过程中需要满足： Openflow 要支持操作 IPv6 拓展头中的相应字段（目前似乎还为支持到这个字段） 仿真网络要使用 IPv6 操作系统要支持 IPv6，并提供相应的操作接口（Linux 内核中已经支持了 IPv6 和 SRv6，但是文档……） 卒。 SR on MPLS SR 在 SDN 中的行为和 MPLS 非常相似，因此在 SDN 中使用 MPLS 来仿真 SR 才是最方便的方案，也是思科在 SR 解析中提倡的思路。因此，你只需要在 mininet 中使用 openvswitch 操作 MPLS 报文即可。 回顾这里提到三类路由策略 (BGP, MPLS, SR) 并非随意枚举，当你将这三者放在一起考虑时，就会发现： 最开始的时候用户啥都不用管，只需要告诉快递公司这个包裹送到 51.51.51.51，快递员就带着包裹逐跳逐跳地为你跑路 后来快递公司开通了专线服务，公司提前耗费人力物力配置好专线，用户的包裹只要符合专线的条件，就交给专线处理，提升速度的同时，却增大了公司专线的运维成本和统一收集信息的成本 之后公司成立了控制中心收集包裹的信息和网络的情况，并在物流网络中设定了一些处理能力比较强的节点，将用户的包裹分散了不同的中转节点进行处理，以此来从整个网络的视角优化网络性能 再出发下面来总结一下： 从控制的层面来看，网络的控制是越来越集中了，更好利用网络资源的同时增大了控制面单点故障的风险 从计算的层面来看，传统网络的分布式计算群体决策演变为集中计算中央决策，群众减负，支持更多的服务，另一方面中央增加了负担，还需要承担整个网络的可靠性风险 从数据的层面来看，数据在网络中的分布可以变的更加的均衡，但是牺牲了一定的时延和速度 那么，这个问题的 trade-off 在哪里呢？区块链技术的发展又是何其相似。 参考资料 https://www.cisco.com/c/zh_cn/solutions/service-provider/segment_routing.html?dtid=odicdc000673#~stickynav=3]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>SDN</tag>
        <tag>Segment Routing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Brief History of SDN]]></title>
    <url>%2F2019%2F03%2F04%2FSDN%2F</url>
    <content type="text"><![CDATA[一直想写一篇关于 SDN 的文章，介绍一下这个技术领域以及…，后来拖了太久，就忘了该怎么写了。 真是个悲伤的故事，就讲一个简单而有趣的故事吧。 Background我们首先简略地回顾一下网络的发展历史： 1962年，美国 MIT 的 Leonard Kleinrock 在其博士论文中首次提出了分组交换的概念，该技术后来成为互联网的标准通信方式 什么是分组交换？为什么需要标准通信方式？ 1969年，美国国防部在“冷战”背景下启动了计算机网络开发计划 ARPANet (Advance Research Projects Agency Network)，标志着现代互联网的诞生 1974年，国际标准化组织 ISO 发布了著名的 ISO/IEC 7498 标准，首次提出并定义了网络分层模型的设计思想，即 OSI (Open System Interconnection) 7-layer Model 为什么要分层设计？这又借鉴了什么？ 1974年12月，Stanford 的 Vinton G. Cerf 和 Robert E. Kahn 领导的研究小组提出了 TCP/IP，利用计算机网络间互联的思想，通过互联不同协议的网络，使构建大规模数据分组网络成为可能 从这里可以知道，IP 是异构网络间的互联，是类似于 Java 虚拟机和 Flash 的存在哦 1983年，ARPANet 宣布将其过去使用的通信协议 NCP (Network Control Protocol) 向 TCP/IP 过渡 1984年，欧洲例子物理研究实验室 （CERN）的 Tim Berners Lee 博士为解决由于 CERN 主机不兼容而无法共享文件的问题，提出了开发一个分布式系统的构想 所以说 HTTP 协议的本质其实就是在异构系统间进行文件共享，是一种文件共享协议，用于传输 HTML 描述的文本文件和媒体文件 1991年， Tim 利用 HTML、HTTP 成功编制了第一个局部存取浏览器 Enguire，从此 Web 开始起飞，并逐步演化为万维网（WWW）技术 为什么 windows 资源管理器和浏览器都叫 explorer？ 1996年，随着万维网的大规模应用，Internet 开始广泛流传 之后，互联网基于 TCP/IP 模型成功容纳了各种不同的底层网络技术和上层应用，风靡全球 可见，互联网初期设计的目标就是要把分散的计算机连接起来以达到资源共享（就是文件和相关的计算资源的共享）的目的。 什么是 SDN可以说，早期互联网关于资源共享的目的已经达到了。最近十年互联网开始利用其共享的资源来提供中心化的服务，产生了大量的互联网服务商。如果你相信区块链的那一套的化，不久的将来可能会出现大量的去中心化的互联网应用和服务。从历史的眼光来看这一过程，就是一个由分到合，由合到分的过程。 在讨论什么是 SDN 之前，让我们来探讨一个问题：你试着去控制过一个网络吗？你能够指导网络中所有快递员的行为和动作，知道网络中的所有状态和服务情况。 SDN 就是这样一种技术，或者说理念——让过去去中心化的、分布式工作的网络设备能够被统一的控制器调配。 be the god. 如果你能控制一个网络，那会发生些什么有趣的事情呢? 为什么需要 SDN事出必有因，SDN 也不例外。传统网络的架构是为了互联互通这个目标而设计的，就简单来说就是够用就好，对于今天各异需求来说我们只能通过修修补补的方式就行满足，你能想想一艘打满补丁的巨轮吗？游轮上有各式各样的需求等待满足。 想要解决这个问题，要么让所有人变得复杂，要么让一个人变得复杂，SDN 选择了后者。 假象有一个计算能力无限的大脑，能够快速响应任何的网络需求并命令网络设备满足该需求，这样的设计之下网络设备就变得非常简单了——只需要努力转发就好了。 SDN 怎么工作要想理解 SDN 怎么工作，还是得知道网络是怎么工作的。 未完待续 Reference SDN Core Principles and Application Practice(3rd edition)]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>SDN</tag>
        <tag>Openflow</tag>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[George Mandy 的架构]]></title>
    <url>%2F2018%2F10%2F16%2Fblog-structure%2F</url>
    <content type="text"><![CDATA[建站思路本站使用了 Github Page + Hexo 的部署思路，这里对本博客的建站思路做一个简单的介绍。 GithubPage 是由著名程序员XX社区 Github 为个人项目提供的静态网站服务，吸引我使用的原因在于：免费、干净、简单。 Hexo 是一个基于XX的 Blog 开发框架，吸引我使用的原因在于：免费、高效、不臃肿。 框架指用于解决某个问题的工具的集合，是问题解决方案的打包，需要使用者在求解时灵活使用工具。 过去我使用过很多 Blog 方案，这里我也对我使用过的方案进行一个简单的总结： WordPress：Blog 建站的一站式解决方案，什么都能干，过于臃肿。 WordPress 之外的建站方案：小众的代价就是折腾，你愿意折腾吗？ Lofter 等博客社交平台：社交是这类平台的核心竞争力，优点在于注册即用，缺点在于你是否符合圈子的口味。 和过去折腾的方案进行比较，我发现 George Mandy 的方案有如下优点： 简洁，轻量 运维轻松 可以自由定制 当然，也有如下缺点： 没有社交能力，如果不推广，谁会知道 ppgp.xyz ？ 静态网站，访问数据及基于数据的交互无法实现，这决定了这个博客只能成为一个展台。 还有一点是我个人的缺点： 基于XX技术的开发能力不足，这决定了我目前无法解决日益增长的阅读体验的要求与网站开发能力不匹配的矛盾，不过，目前就这样吧。 建站理念George 想要构建一个分享自己思考的地方，人类社会是复杂而精巧的，也是简单且高效的，我希望在复杂的内容中探索它那些简单的内核。 同时，我想打造一种阅读体验——灯下阅读。这需要我对 Hexo 主题系统的样式模块进行全面的改造，诶，慢慢来喽。 博客架构我到目前为止没有系统学习过架构的知识，但是我希望在这里提供我这个 Blog 架构设计的一些思路，供大家参考。 如果称得上是架构的话，我相信读者的你很少会见到博主谈这方面的内容，但是我觉得这么简单的东西里面还是有很多有趣的细节值得玩味。 先看看我们的 Big Picture：一个完全架构在 Github 上的博客系统！ 这个博客有三种用户权限： 创作者：基于 MyPosts 仓库专注于创作，并借助 Git 的版本控制能力和 Markdown 高效的样式控制能力获得前所未有创作体验。 管理者：负责 Blog 的运维，定时从 MyPosts 仓库获取内容并发布到 GithubPage 仓库。 游客：通过 ppgp.xyz 访问博客。 详细点来说，整个 Blog 的运行过程是这样的： 创作者 clone MyPosts 仓库到本地，新建自己的分支进行的创作，之后提交分支到 GitHub； 管理者 检查 MyPosts 仓库的情况，审核分支内容，通过的分支合并到主线，之后拉取更新的内容和 Hexo 框架文件和配置文件在本地更新配置，最后部署到 GithubPage 仓库； 游客自主通过 ppgp.xyz 访问 GithubPage 上的页面。 其他优势是的，这套架构还能轻松地拓展出下面的能力： 多创作者模式：完全依托 Github 提供的分布式内容管理能力； 多平台编辑、运维：需要的时候就把仓库拉下来，改好了就推上去，本地不需要保存任何内容； 需要解决的问题 每个仓库的管理由 Github 完成，但仓库与仓库之间的通信呢？ 管理者是唯一能够访问三个仓库的人，因此管理者成为了三个仓库间通信的管道。 创作者不会使用 Git？ 从创作者的角度来说，Git 提供的版本管理能力是他们从没有体验过的生产力工具，但是不得不说，对小白还是太不友好了，所有有必要将内容和版本管理这部分进行二次封装，提供一个直白的使用方式。 MyPosts 到 Hexo 是如何更新的？ 简单点，你可以直接将 Hexo/source/_posts 复制到 MyPosts里去，每次从 MyPosts 复制过去就好了 当然为了更方便地创作，你可能需要设计一个 Markdown 模板生成器和内容更新器。 可以在静态博客上实现评论吗？ 当然是可以的，只要你愿意折腾 总结一句话：由 Github 全托管，基于 Hexo框架并提供内容版本管理能力的静态博客系统。]]></content>
      <categories>
        <category>效率工程</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>架构</tag>
        <tag>GithubPage</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[画家和藏品]]></title>
    <url>%2F2018%2F10%2F08%2Fintroduce-to-nft%2F</url>
    <content type="text"><![CDATA[以太王国中有一位油画家 Alice，还有两位油画收藏家 Bob 和 Charles，他们喜欢 Alice 的油画并乐于收藏和交易，油画家 Alice 为了方便大家收藏和交换他的油画，在以太王国中针对他的油画提供了一些服务： 人们可以到 Alice 这里查询自己是否拥有 Alice 的油画，拥有多少幅以及分别是哪些油画 人们可以到 Alice 这里查询每幅油画的主人 人们可以使用 Alice 提供的服务对油画进行所有权的授权 人们可以使用 Alice 提供的服务转移油画的所有权 Alice 负责记录与自己油画相关的所有所有权信息 我们假设以太王国所有人都是值得信任的。在故事的开始，Bob使用以太王国的法币——以太币（ETH）向 Alice 购买了油画 001号，Charles 使用同样的方式购买了 002 号。Alice 在收到二者的付款证明和将自己手上 001 和 002 的所有权转移给了 Bob 和 Charles。 油画转移服务：在这里，我们知道了以太王国中收藏品交易的基本手段：使用以太币向藏品所有权持有人购买其藏品的所有权，此时藏品所有权立即转移给买方。 由于这是系统中藏品产生后的藏品发生的第一笔交易（第一次进入流通领域），我们不妨把它叫做藏品的发行。 有一天 Charles 到 Bob 家做客，Charles 发现了 Bob 家墙上的 002 号油画，Charles 非常喜欢这幅画并希望 Bob 将这幅画卖给他。Bob 觉得 Charles 给的价格不错，便同意了这次交易（交易撮合），他们便到 Alice 处办理藏品所有权转移的手续：Bob在私下将约定数量的以太币转账给 Charles，Charles 在 Alice 处签署将藏品所有权转移给 Bob 的手续。手续签署完成（交易验证完成）后 002 号油画的所有权就属于 Charles 了，此时 Charles 为了查询自己是否真的拥有 002 号油画，他跑到 Alice 家去查询自己账户下的藏品情况和藏品数量，Alice 告诉他如下信息： 12Charles.num(NFT): 2Charles.NFT:[001, 002] Charles 对查询结果很满意，但他长了个心眼，他想看看 Bob 是否还拥有 002 号的所有权，他刚好知道 Bob 的账号信息，于是他继续向 Alice 询问了相同的信息，Alice 告诉他： 12Bob.num(NFT): 0Bob.NFT: [] OK，002 号目前的确是独一无二且只属于 Charles 的，不过多疑的 Charles 又长了个心眼，他想看看 Alice 还能不能画一幅一摸一样的 002 号，他问 Alice道： Charles: Alice，你现在还能画一幅 002 号油画吗？ Alice: 对不起，为了保证我的油画的收藏品属性，所有的油画都是独一无二的，除非 002 号被销毁了，否则我是不能创造出新的 002 号油画的。 Charles：那你目前画了多少油画了？ Alice：2幅。 Charles：那你还会再画更多的油画吗？ Alice：当然，只要有需要我就会画更多的油画，当然每幅油画都是不同的。 Charles 自从得到了 002 号油画后，非常欣赏 Alice 的油画能力，于是他又向 Alice 购买了接下来的 003~010 七幅油画，一方面是自己喜欢，一方面是相信 Alice 油画的增值能力。 又是一天，Bob 到 Charles 家做客，Bob 发现了 Charles 家的 003 号油画，同样的故事发生了，Bob 希望买下Charles 手中的这幅画，精明的 Charles 和 Bob 进行了激烈的议价环节最终达成了交易意向，此时 Alice 提供一一项新的服务： 油画授权服务：油画所有人可以向 Alice 寄送一封签名信，将油画的所有权转移给其他人，得到授权的人可以自行到 Alice 处获取油画的所有权。 Charles 使用了这项服务，在确认了 Bob 的付款信息后给 Alice 写信对 Bob 授予了 003 号油画的所有权。Bob 可以日后择日去 Alice 处转移 003 号的所有权，需要注意的是，在 Bob 去办理转移之前，003 号的所有权依然是 Charles 的。 Charles 通过这次交易发现 Alice 的油画确实有价值，而 Charles 手中有大量的油画，自己亲自去市场上寻找交易对象费时又费力，于是 Charles 在以太王国中找到了专业的资产管理员 David，Charles 希望将自己手中所有的油画授权给 David 管理，由 David 去市场上进行交易。Charles 去 Alice 处办理所有权移交手续，他发现使用刚才的每幅油画的授权方式很麻烦，于是他问 Alice 能不能直接授权我整个账户上收藏品的所有权？，刚好 Alice 提供了这项服务，于是 Charles 将账户上所有油画的所有权授予了 David 进行操作（David 此时可以称作 Charles 藏品的操作员，Operator），此时 Charles 手中的油画其实有两个具有所有权操作能力的操作员：Charles 和 David。 一点小思考 Charles 将油画授权给 Bob 之后，Charles 能对被授权的油画进行操作吗？ ANS：答案是可以的，在ERC-721 协议的定义中，Charles 对油画的授权是所有权的复制而非转移，Charles 是可以将一幅画授权给一个人的同时又立即卖给另一个人的（只要 Bob 不立即转移油画的所有权）。 那么这一点应该看作是协议的特性还是协议的 BUG 呢？ 这个问题从 ERC-20 协议开始官方就不认为是一个 BUG : )，但是也有一部分人认为这是一种 BUG。换个角度来想，在交易双方都可信的情况下这不能算一个 BUG，但在现实世界中，有了利益的驱动，你懂的。 为什么以太坊需要ERC-721协议，ERC-20不是挺好的吗？ 首先 ERC-20 是一个很成功的协议，至少引爆了 ICO 的浪潮和我们国家的严厉监管 : )，ERC-20 Token 从服务代币的角度来讲已经能够满足基本需要了（作为一个普适性的协议没有必要考虑所有的使用情况），对于现实世界中可以计数的产品或服务（比如 USDT，将美元和Token绑定），ERC-20 Token 可以作为一个很好的映射，解决其数字化和流通的问题，但是现实世界中另一部分很重要的东西——资产，特别是独一无二的资产，使用 ERC-20 Token 来表示是不合适的（你的房子价值 100 个 USDT，我的房子也值 100 个 USDT，那我们的房子一样吗？），因此我们需要一种能够映射不同资产的 Token，它满足： 每个 Token 都是独一无二的（房子与房子之间是不同的）； 同一个人发行的 Token 之间是可以转移所有权的（我能将我的房子的所有权转移给你）； Token 之间本身是不能交换的（我给你一个我的房子，你也给我一个你的房子，这样是不行的）。 这种资产映射的需求是巨大且充满想象的，至少它赋予了那些值钱的庞然大物两个特点：可分割性和流动性。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>加密经济学</tag>
        <tag>EIP-721</tag>
        <tag>NFT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现金、支票与银行账户]]></title>
    <url>%2F2018%2F10%2F06%2Fpay%2F</url>
    <content type="text"><![CDATA[在很久很久以后，人类文明只剩下一个国家——以太王国，这个王国借助区块链技术发展了非常发达的数字经济，发行的法币名字叫 ETH，另一方面，它的公民全都使用在线支付。Alice，Bob，Charles 是王国的三位公民，不出意外的话他们会是以后所有系列文章的主角。对了，本文还需要一位万能的角色：God，用于在人民群众的监督下优化王国的金融系统。 王国中所有人的名字就是他们的账户名，所有的公民使用一个统一的账本来记录自己的账户余额，这个账本每个人都能抄一份保存在自己家里，账本表示如下： 1ETH_balances[usr_name] = XXX ETH 在一切开始之前，在一切开始之前，让我们先看看大家的账户情况： 1234ETH_balances[Alice] = 1 ETHETH_balances[Bob] = 1 ETHETH_balances[Chales] = 1 ETHETH_balances[God] = 10000 ETH Account Alice Bob Charles Symbol: ETH 1 1 1 God 计划借助区块链在 Alice、Bob、Charles 三个人之间发行一种除 ETH 以外的代币来奖励三者之间的友好行为，God设计的代币方案如下： 代币的名字叫 Good Friend Token，代币缩写为 GFT，发行量为 100 枚，最小发行单位为1枚。 God 设计好代币方案后又制定了发行计划： 由 God 将代币方案公布给大家，同时将初始的 100 个 GFT 全部交给 God，由 God 依照大家的友好程度分发给大家。 为了保证大家能够自由地使用 GFT，同时又不希望设计太复杂的金融功能来增加自己的工作量，God在观察人类社会的货币体系后总结出了下面两条必备的功能： 用户能够查询自己代币账户的余额； 用户能够在自己的代币账户之间转账。 OK，万事俱备，God 愉快地在Alice、Bob、Charles 三个人之间发行了 GFT 代币，具体的发行过程如下： God 在全国范围内宣布了这个方案，并为这个方案成立了一个银行，银行名字叫 GBT_Bank，它也有一个账户 GBT_Bank 银行按照 God 设计的发行计划，在银行管理的 GFT账本上将初始的 100 GFT 全部划到 God 名下： 1GFT_balances[God] = 100 GFT 剩下的人可以到 GFT_Bank 开户以便查询和交易自己账户上的 GFT。Alice、Bob、Charles 得知这一情况后也立即到银行开了户，查询他们的账户情况如下： 123GFT_balances[Alice] = 0 GFTGFT_balances[Bob] = 0 GFTGFT_balances[Chales] = 0 GFT | Account | Alice | Bob | Charles || :———: | :—: | :–: | :—–: || Symbol: GFT | 0 | 0 | 0 | 此时该系统中有了货币和账户，可以开始设计流通方法啦。 为了方便大家在线获取自己的账户信息，GFT_Bank 为这一代币设计了两个操作方法，大家在自己的电脑上运行这两个方法就能实现对自己账户的操作： 查询操作：由用户发起，查询 GFT 账本上对应账户的余额，由于以太王国所有账户都是公开的，任何人都可以查到其他人的账户信息： 12Alice.balanceOf(Alice) = 0 GFTAlice.balanceOf(God) = 100 GFT 转账操作：由转账人发起，将转账人账户上的一些代币转入目标账户中，转账完毕后可以查询对应账户的余额信息： 1234God.transfer(Alice, 10)Alice.balanceOf(Alice) = 10 GFTAlice.balanceOf(God) = 90 GFT | Account | Alice | Bob | Charles || :———: | :—: | :–: | :—–: || Symbol: GFT | 10 | 0 | 0 | OK，有了这两个功能，大家就可以愉快地转账啦，整个系统也能够运转起来了。 以上，就是我认为在一个特定空间中发行代币的最小功能集合，这已经能够保证系统运行起来了。在进入更加深入的分析之前，我们先来探讨几个问题： 每个人的账户是怎么来的？ 在以太王国中，由于 God 精妙的安排，每个人的名字都是独一无二的，因此我们可以直接以每个人的名字设计账户，同时所有账户 ID 都向社会公开 账户会不会被冒用？ 由于 God 精妙的安排，以太王国的银行系统保证了自己的账户只能被自己打开和使用 为什么我的默认账本（ETH_balances）查询不到我的 GFT 信息？ GFT 是由 GFT_Bank 发行的，换言之，其他银行是不承认这个代币的，因此你的一切操作只能去 GFT_Bank 办理。 为什么发行的 GFT 必须到 GFT 开户，并借助 GFT 才能操作？ 其实不管你是否去GFT 开户，理论上 GFT_Bank 都预留了你的账户信息，因为你的账户 ID 是公开的，只不过如果你不去 GFT_Bank 进行绑定，你就不知道你的 GFT 账本上有多少 GFT，也不可能去花你的 GFT。从另一个角度来讲，假如你没有去 GFT_Bank 开户（绑定），而你的 GFT 账户又默认存在，如果哪个小马虎错误地给你转了一笔 GFT，对你而言也只是一串数字而已。 由于只有两个功能：查询和转账，整个 GFT 系统运行地很好，但是随着大家金融活动的繁荣，人们对 GFT 提出了新的需求：我们能不能在 GFT 体系里使用支票？ 在这里我们先不讨论支票这一功能背后精妙的金融创造，我们先和 God 一起去满足这一需求。 God 的开发原则依旧：在满足需求的情况下作最简单的实现。God 用他独有的上帝视角继续在人类社会中寻找方案，最终总结出下面三个功能： 转出账户向接收账户授予一定额度； 1Alice.approve(Bob, 5) 接收账户从转出账户转移一定额度； 1Bob.transferFrom(Alice, Bob, 5) 查询账户之间的支票配额信息。 1Bob.allwance(Alice, Bob) GFT_Bank 为了实现这三个功能，光靠 GFT_balances 账本可不行，于是 GFT_Bank 又引入了一个新的账本：GFT_Allowced，在这里我们不妨称其为支票账本，它长这样的： From \ To Alice Bob Charles Alice 0 0 0 Bob 0 0 0 Charles 0 0 0 此时的余额账本信息： Account Alice Bob Charles Symbol: GFT 10 0 0 我们通过一个简单的例子来了解这一过程是如何进行的： Bob 帮了 Alice 一个忙， Alice 对 Bob 承诺说：Bob，我从我的账户账户上给你开 10 GFT 的支票，你什么时候去取都可以； 1Alice.approve(Bob, 10) From \ To Alice Bob Charles Alice 0 10 0 Bob 0 0 0 Charles 0 0 0 Bob 听了之后很高兴，赶紧去 GFT_Bank 的支票配额账本上查询 Alice 说的是不是真的； 1Bob.allwance(Alice, Bob) Bob 查询之后并没有立即将 GFT 转入自己名下； 几天之后，Bob遇到急事突然需要 5 GFT 进行周转，他想到了 Alice 的支票，于是去 Alice 账户上提取了 5 GFT 出来，并查询了之后的支票配额情况。 1Bob.transferFrom(Alice, Bob, 5) 此时的余额账本信息： From \ To Alice Bob Charles Alice 0 5 0 Bob 0 0 0 Charles 0 0 0 Account Alice Bob Charles Symbol: GFT 5 5 0 至此，GFT_Bank 就能满足广大人民群众日益增长的转账需要了。 故事讲完了，细心的你可能会有一个疑问：为什么是支票呢？]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>加密经济学</tag>
        <tag>ERC-20</tag>
      </tags>
  </entry>
</search>
